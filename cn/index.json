[{"body":"强化学习理论基础\n背景 强化学习（Reinforcement Learning, RL）是机器学习中一个强大且快速发展的分支，受行为心理学的启发。它主要研究智能体如何在环境中采取行动，以最大化某种累积奖励的概念。与监督学习不同，监督学习中的学习智能体是给定输入-输出对，而强化学习则强调通过交互进行学习。\n强化学习（RL）在各种领域的应用至关重要。在机器人技术中，它驱动自主导航和物体操作任务；在游戏领域，如AlphaGo展示了其在战略决策方面的卓越能力。RL优化金融交易策略，调整医疗保健治疗方案，并增强自然语言处理和计算机视觉的能力。其应用范围还扩展到智能电网、推荐系统和虚拟助理等领域，凸显了RL在各个领域中的变革性影响。\n理论基础 强化学习（RL）问题旨在通过与环境 $\\mathcal{E}$ 的交互，在多个离散时间步中实现学习以达到某个目标。在每个时间步 $t$，智能体接收环境状态空间 $\\mathcal{S}$ 中的一个状态 ${s}{t}$，并根据策略 $\\pi({a}{t}|{s}{t})$ 选择一个动作 $a_t \\in \\mathcal {A}(s_t)$，其中 $\\mathcal{A}(s_t)$ 是状态 $s_t$ 下可用动作的集合。策略表示为条件概率 $\\pi(a|s)$，即当前状态为 $s$ 时智能体采取动作 $a$ 的概率。这是从状态和动作到采取某动作概率的映射。之后，智能体会收到一个标量奖励 ${r}{t}$ 并将转移存储在智能体的记忆中作为经验。这一过程持续进行，直到智能体达到终止状态。智能体的目标是学习一个策略 ${ \\pi }^{ \\ast }$，使期望折扣回报 ${ R }_{ t }=\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }$ 最大化，其中折扣因子 $\\gamma \\in (0,1]$ 用于权衡即时奖励和未来奖励的重要性。\n满足马尔可夫性质的强化学习任务可以描述为马尔可夫决策过程（MDPs），其定义为一个五元组 $(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma)$，其中 $\\mathcal{R}$ 是奖励函数 $\\mathcal{R}(s,a)$，$\\mathcal{P}$ 是状态转移概率 $\\mathcal{P}({s}{t+1}|{s}{t},{a}{t})$。马尔可夫性质表明，在给定当前状态的情况下，未来状态与过去状态条件独立。因此，在强化学习任务中，决策和价值被假定为仅是当前状态的函数。马尔可夫性质可以定义为 $p({ s }{ t+1 }|{ s }{ 1 },{ a }{ 1 },...,{ s }{ t },{ a }{ t }) = p({ s }{ t+1 }|{ s }{ t },{ a }{ t })$，这意味着在给定当前状态的情况下，未来状态与过去状态条件独立。满足马尔可夫性质的强化学习任务可以描述为马尔可夫决策过程，其定义为五元组 $(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma)$，其中 $\\mathcal{R}$ 是奖励函数 $\\mathcal{R}(s,a)$，$\\mathcal{P}$ 是状态转移概率 $\\mathcal{P}({s}{t+1}|{s}{t},{a}{t})$。在一个情节任务中，每个情节长度结束后状态会重置，一个情节中的状态、动作和奖励序列构成了策略的轨迹或展开。\n价值函数 价值函数是强化学习系统的核心组件，它构建了一个函数逼近器，用于估计任一状态的长期奖励。它估计智能体处于给定状态（或在给定状态下采取给定动作）时的好坏程度（预期回报）。通过这种方式，函数逼近器利用状态空间的结构来有效地学习观察到的状态的价值，并推广到类似的、未见过的状态的价值。一个典型的价值函数形式可以定义为：\n$${ V }^{ \\pi }(s)=\\mathbb{ E }[R|s,\\pi ]= \\mathbb{E}[\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }|s,\\pi] $$\n通常我们将 ${ V }^{ \\pi }(s)$ 称为状态价值函数，它衡量在状态 $s$ 开始并遵循策略 $\\pi$ 时的预期折扣回报。当动作遵循最优策略 ${\\pi}^{\\ast}$ 时，状态价值函数可以达到最优：\n$${ V }^{ \\ast }(s)=\\max _{ \\pi }{ { V }^{ \\pi }(s) } \\quad \\forall s\\in \\mathcal{ S }$$\n除了衡量状态的价值之外，还有一个用于衡量动作选择质量的指标，称为状态-动作价值或质量函数 ${Q}^{\\pi}(s,a)$。它定义了在给定状态 $s$ 下选择动作 $a$ 并随后遵循策略 $\\pi$ 的价值。\n$${ Q }^{ \\pi }(s,a)=\\mathbb{ E }[R|s,a,\\pi ]= \\mathbb{E}[\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }|s,a,\\pi] $$\n状态-动作价值与状态价值 $V^{\\pi}$ 类似，不同之处在于提供了初始动作 $a$，并且策略 $\\pi$ 仅从随后的状态开始执行。最优状态-动作价值函数表示为：\n$${ Q }^{ \\ast }(s,a)=\\max _{ \\pi }{ { Q }^{ \\pi }(s,a) } \\quad \\forall s\\in \\mathcal{ S } , \\forall a\\in \\mathcal{ A } $$\n${ Q }^{ \\ast }(s,a)$ 给出了状态 $s$ 和动作 $a$ 的最大状态-动作价值，这是任何策略可以达到的值。\n这个动作价值函数满足递归性质，这是强化学习设置中价值函数的一个基本属性，它表达了状态值与其后继状态之间的关系：\n$${Q}^{\\pi}(s,a)=\\mathbb{E}{{s}^{\\prime}}[r+\\gamma\\mathbb{E}{{a}^{\\prime}\\sim{\\pi}({s}^{\\prime})}[{Q}^{\\ast}({s}^{\\prime},{a}^{\\prime})]|s,a,\\pi] $$\n与生成绝对状态-动作值 $Q^{\\pi}$ 不同，优势函数表示相对状态-动作值，衡量动作是否比策略的默认行为更好或更差。通常，学习某个动作产生更高奖励比学习采取特定动作的实际回报更容易。优势函数通过以下简单关系表达动作的相对优势：\n$${ A }^{ \\pi }(s,a)={ Q }^{ \\pi }(s,a)-{ V }^{ \\pi }(s) $$\n许多成功的基于值的强化学习算法依赖于优势更新的思想。\n核心强化学习算法 深度 Q-网络 深度强化学习（DRL）应用深度神经网络来表示强化学习方法中的价值函数。DRL算法在多个挑战性任务领域取得了超人类表现，这归功于深度学习强大的函数逼近和表示学习能力。DQN算法在Atari系列游戏中通过像素输入达到了人类水平的表现。它使用神经网络 $Q(s,a;\\theta)$ 参数化质量函数 $Q$，从而逼近 $Q$ 值。DQN算法的两个主要技术，即使用目标网络和经验回放，可以稳定和有效地学习价值函数。在每次迭代中，网络的参数更新通过最小化以下损失函数来实现：\n$${L}{i}({\\theta}{i})=\\mathbb{E}{s,a,r,{s}^{\\prime}}[({y}{i}^{DQN}-Q(s,a;{\\theta}_{i}))^{2}]$$\n其中\n$${y}_{i}^{DQN}=r+\\gamma \\underset {{a}^{\\prime}}{max}Q({s}^{\\prime},{a}^{\\prime};{\\theta}^{-}) $$\n在其中，${\\theta}^{-}$ 是目标网络的参数。第一稳定方法是固定目标网络的参数，而不是基于其自身快速波动的$Q$值估计来计算TD误差。第二个方法是经验回放，它使用缓冲区存储一定大小的转换$({s}{t},{a}{t},{s}{t+1},{r}{t+1})$，可以进行离策略训练，并增强数据采样的效率。\n在DQN算法引发这一领域之后，价值基础的强化学习设置经历了一系列改进。为了减少DQN中被高估的$Q$值，van Hasselt等人提出了双重DQN算法。Wang等人提出了一种对抗Q网络架构，用于分别估计状态值函数$V(s)$和相关的优势函数$A(s,a)$。Tamar等人提出了一种价值迭代网络，能有效地学习规划，并在许多强化学习任务中实现更好的泛化。Schaul等人开发了建立在双重DQN之上的PER方法，它使经验回放过程比统一重播所有转换更加高效和有效。\nDueling 网络结构 与标准的单序列$Q$网络设计不同，对抗网络结构包括两个序列（流）的网络（A网络和V网络），分别学习动作优势函数和状态值函数。这种构造将价值函数和优势函数解耦，并结合这两个流来产生状态-动作值函数的估计，使用特殊的聚合模块。这两个流共享一个公共的特征提取层（或较低层）。深度$Q$网络专注于估计每个状态-动作对的价值。然而，对抗网络的想法是分别估计与动作无关的状态函数和依赖于动作的优势函数，因为在强化学习环境中，并非所有状态都与特定动作相关，有许多状态是与动作无关的，在这些状态下，智能体不需要改变动作来适应新的状态。因此，估计这些状态-动作对的值是无意义且低效的。对抗网络首次由Wang等人提出，通过这一改变，训练效率比单一流$Q$网络大大提高。根据Wang的工作，对抗网络在离散动作空间任务中取得了新的技术成果。简而言之，对抗网络生成的$Q$值对性能提升比深度$Q$网络在强化学习任务中更有优势。\n策略梯度 上述方法间接地通过估计值函数学习策略 $\\pi(s)$。这些基于值的方法在处理离散动作领域的问题时非常有效。然而，当处理具有连续动作空间的问题，如物理控制任务时，基于值的方法不能直接应用，并且很难确保结果的收敛性，因为它依赖于每个动作的$Q$值。将基于值的算法（如DQN）应用于连续域的一个显而易见的方法是将动作空间离散化为若干固定动作。然而，这种方法有许多缺点和局限性，如可能丢失关于动作域结构的重要信息。\n在基于策略的方法中不存在这种担忧，因为策略网络直接输出智能体的动作，而无需估计动作值函数。它们直接参数化控制策略 $\\pi(a|s;\\theta)$ 并更新参数 $\\theta$ 来优化累积奖励，因此，基于策略的方法比基于值的方法更适用于连续控制问题，如机器人控制任务。\n策略梯度（PG）是一种吸引人的基于策略的算法，它优化参数化策略 ${\\pi}{\\theta}(a|s)=\\mathbb{P}[a|s;\\theta]$，按照策略参数的期望累积奖励梯度 ${\\nabla}{\\theta}J({\\pi}{\\theta})$ 进行优化。策略梯度方法在高维或连续动作空间中非常有效，并且可以学习到随机策略。在强化学习任务中，智能体的目标是找到最大化目标函数 $J(\\pi)$ 的参数 $\\theta$。一个典型的性能目标是平均奖励函数：$J(\\pi)=\\mathbb{E}[R|{\\pi}{\\theta}]$。策略梯度定理提供了关于策略 $\\pi$ 参数 $\\theta$ 的梯度：\n$${\\nabla}{\\theta}J({\\pi}{\\theta})=\\int {\\mathcal{S}}^{ }{{\\rho}^{\\pi} }\\int{\\mathcal{A}}^{ }{{\\nabla}{\\theta}}{\\pi}{\\theta}(a|s){Q}^{ \\pi}(s,a)dads \\ \\quad\\quad\\quad\\quad=\\mathbb{E}{s\\sim{\\rho}^{\\pi},a\\sim {\\pi}^{\\theta}}[{\\nabla}{\\theta} log{\\pi}^{\\theta}(a|s){Q}^{\\pi}(s,a)] $$\n其中，${\\rho}^{\\pi}(s)$ 是状态分布。未知部分 ${Q}^{\\pi}(s,a)$ 通常通过使用实际回报 ${ R }_{ t }=\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }$ 作为每个 ${Q}^{\\pi}(s_t,a_t)$ 的近似来估计。基于这个定理，Silver等人提出了确定性策略梯度（DPG）算法用于梯度估计，它比通常的随机策略梯度方法更有效。O'Donoghue等人提到了一种结合PGQL的新技术，并讨论了在强化学习环境中实现这一技术的实际方法。\nActor-Critic 算法 常规的策略梯度方法由于梯度估计的方差较大，通常表现出收敛缓慢的特点。演员-评论家方法通过引入评论家网络来估计当前策略的值，试图通过这种方式减少方差，并利用这些估计值来更新演员策略参数，以提升性能。\n动作选择策略称为演员 ${\\pi}{\\theta}:\\mathcal{S}\\rightarrow \\mathcal{A}$，它在不需要对值函数进行优化的情况下做出决策，将状态表示映射到动作选择概率上。值函数称为评论家 ${Q}{\\phi}^{\\pi}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$，它估计预期回报以减少方差并加速学习，将状态映射到预期的累积未来奖励上。\n演员和评论家是两个独立的网络，共享相同的观察。在每个步骤中，演员网络选择的动作也是评论家网络的输入因子。在策略改进过程中，评论家网络通过DQN估计当前策略的状态-动作值，然后演员网络根据这些估计值更新其策略以改进$Q$值。相比于以前的纯策略梯度方法，使用评论家网络来评估当前策略更有助于收敛和稳定性。状态-动作值评估越好，学习性能的方差就越低。在评论家网络中进行更好的策略评估是重要且有帮助的。\n基于策略梯度的演员-评论家算法在许多现实应用中非常有用，因为它们可以使用低方差的梯度估计来寻找最优策略。Lillicrap等人提出了DDPG算法，将演员-评论家方法与DQN的见解结合起来，解决了模拟物理任务，并广泛应用于许多机器人控制任务。该算法使用两个神经网络：演员网络学习确定性策略，评论家网络逼近当前策略的Q函数。\n总结 强化学习（RL）代表了机器学习领域的一个强大范式，灵感来自行为心理学，使得智能体能够在复杂环境中做出决策，以最大化累积奖励。作为马尔可夫决策过程（MDP）形式化，RL任务涉及状态、动作、奖励和转移概率。例如，深度 Q 网络（DQN）等算法利用深度神经网络高效逼近 Q 值，从而在离散动作空间中促进决策制定。\n基于价值的RL方法，例如DQN，通过估计状态-动作值来优化策略。演员-评论家方法改进了传统的策略梯度方法，引入评论家网络来估计值函数，从而减少方差并增强学习稳定性。这些进展通过像深度确定性策略梯度（DDPG）这样的算法扩展到连续动作空间，结合了确定性策略和 Q 函数逼近。\n策略梯度方法直接基于预期奖励的梯度估计优化策略，在连续动作空间中表现出色。对于提高训练效率的一个重要贡献是 dueling 网络结构，它分离了状态值和优势函数，重视依赖于动作的优势。\n总体而言，强化学习通过价值估计、策略优化以及在机器人学和游戏玩法等多样领域的应用中的创新不断发展。神经网络架构和学习算法的进步持续推动着强化学习研究和应用的进步。近年来的趋势包括将 RL 适应连续动作空间，与自然语言处理和计算机视觉等领域融合，提高采样效率和训练稳定性。未来的方向包括通过跨学科合作提升 RL 在解决现实挑战中的适用性，并解决部署中的伦理考量。\n","link":"https://mengwoods.github.io/cn/post/dl/008-drl-summary/","section":"post","tags":null,"title":"强化学习理论基础介绍"},{"body":"基于 Deep Learning (2017, MIT) 书.\n1 概述 概率论是表示不确定性陈述的数学框架。在AI领域中，我们以两种主要方式使用概率论。首先，概率定律告诉我们AI系统应该如何推理，因此我们设计算法来计算或近似使用概率论得出的各种表达式。其次，我们可以使用概率和统计来理论上分析所提出的AI系统的行为。\n2 知识 2.1 离散变量和概率质量函数（PMF） 对离散变量的概率分布可以用概率质量函数（PMF）来描述。 离散变量$x$遵循分布$P(x)$:$\\mathrm{x}\\sim P(x)$。\n联合概率分布是许多变量的概率分布：$P(\\mathrm{x}=x, \\mathrm{y}=y)$，或者$P(x,y)$。\nPMF的特性：\n$P$的定义域必须是$\\mathrm{x}$的所有可能状态的集合。 对于$\\forall x\\in \\mathrm{x}$，$0\\leq P(x) \\leq 1$。 $\\sum_{x\\in \\mathrm{x}}P(x)=1$。 均匀分布：$P(\\mathrm{x}=x_i)=\\dfrac{1}{K}$。\n2.2 连续变量和概率密度函数（PDF） 概率密度函数（PDF）用于描述连续随机变量的概率分布。PDF的函数$p$必须满足以下特性：\n$p$的定义域是$\\mathrm{x}$的所有可能状态的集合。 对于$\\forall x\\in \\mathrm{x}$，$p(x)\\geq0$。注意不要求$p(x)\\leq 1$。 $\\int p(x)dx=1$。 PDF不是概率，PDF与PMF不同，PDF可以大于1。离散和连续随机变量的定义方式不同。对于连续随机变量，必要条件是$\\int p(x)dx=1$。PDF不直接给出特定状态的概率，而是给出落入$\\delta x$的无穷小区域内的概率，即$p(x)\\delta x$。变量$x$位于区间$[a,b]$的概率由$\\int_{[a,b]}p(x)dx$给出。\n均匀分布 $u(x;a,b)=\\dfrac{1}{b-a}$，$a$和$b$是区间的端点。分号表示参数化。$x$是函数的参数，$a$和$b$是参数。$x\\sim U(a,b)$表示$x$遵循均匀分布。\n2.3 边缘概率 对变量子集的概率分布称为边缘概率分布。例如，对于离散随机变量$\\mathrm{x}$和$\\mathrm{y}$，已知$P(\\mathrm{x},\\mathrm{y})$，可以使用求和规则计算$P(\\mathrm{x})$：$\\forall x\\in \\mathrm{x}$，$P(\\mathrm{x}=x)=\\sum_{y}P(\\mathrm{x}=x, \\mathrm{y}=y)$。对于连续变量，需要使用积分而不是求和：$p(x)=\\int p(x,y)dy$。\n2.4 条件概率 计算某个事件发生的概率，已知某些其他事件已发生。这是条件概率。$P(\\mathrm{y}=y|\\mathrm{x}=x)$，$\\mathrm{x}=x$是条件。可以使用公式$P(\\mathrm{y}=y|\\mathrm{x}=x)=\\dfrac{P(\\mathrm{y}=y,\\mathrm{x}=x)}{P(\\mathrm{x}=x)}$来计算。\n条件概率仅在$P(\\mathrm{x}=x)\u0026gt;0$时定义。我们不能计算条件是从不发生事件的条件概率。\n2.5 条件概率的链式法则 任何多个随机变量的联合概率分布可以分解为对单个变量的条件分布，这称为链式法则或乘法规则。$P(\\mathrm{x}^{(1)},\\ldots,\\mathrm{x}^{(n)})=P(\\mathrm{x}^{(1)})\\Pi_{i=2}^nP(\\mathrm{x}^{(i)}|\\mathrm{x}^{(1)},\\ldots,\\mathrm{x}^{(i-1)})$。\n一些例子：\n$P(a,b,c)=P(a|b,c)P(b,c)$;\n$P(b,c)=P(b|c)P(c)$;\n$P(a,b,c)=P(a|b,c)P(b|c)P(c)$。\n2.6 独立性和条件独立性 如果$x$和$y$是独立的（$x\\perp y$），则：$\\forall x\\in \\mathrm{x}, y \\in \\mathrm{y}, p(\\mathrm{x}=x, \\mathrm{y}=y)=p(\\mathrm{x}=x)p(\\mathrm{y}=y)$。\n给定随机变量$z$，如果$x$和$y$在条件$z$下独立（$x\\perp y|z$），则：\n$\\forall x\\in \\mathrm{x}, y\\in \\mathrm{y}, z\\in \\mathrm{z}, p(\\mathrm{x}=x, \\mathrm{y}=y, \\mathrm{z}=z)=p(\\mathrm{x}=x|\\mathrm{z}=z)p(\\mathrm{y}=y|\\mathrm{z}=z)$\n2.7 期望、方差和协方差 期望\n对于离散变量：$\\mathbb{E}{\\mathrm{x}\\sim P}[f(x)]=\\sum{x}P(x)f(x)$。\n对于连续变量：$\\mathbb{E}_{\\mathrm{x}\\sim P}[f(x)]=\\int{P(x)f(x)}dx$。\n期望是线性的：$\\mathbb{E}{\\mathrm{x}}[\\alpha f(x)+\\beta g(x)]=\\alpha \\mathbb{E}{\\mathrm{x}}[f(x)] + \\beta \\mathbb{E}_{\\mathrm{x}}[g(x)]$\n方差\n$Var(f(x))=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]$\n当方差很小时，$f(x)$的值会聚集在其期望值附近。方差的平方根称为标准差。\n协方差\n协方差给出两个值之间线性相关的程度，以及这些变量的尺度：\n$Cov(f(x),g(y))=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])(g(y)-\\mathbb{E}[g(y)])]$\n协方差的绝对值较高意味着这些值变化很大，并且同时远离各自的均值。正号表示两个变量倾向于同时取相对较高的值。负号表示一个变量取得高值，另一个变量取得低值，反之亦然。\n协方差与相关的关系：\n独立变量的协方差为零。非零协方差的变量是相关的。 独立性是比零协方差更强的要求。两个变量可以相关，但协方差为零。 随机向量$\\mathbf{x}\\in \\mathbb{R}^n$的协方差矩阵是一个$n\\times n$矩阵：$Cov(\\mathbf{x})_{i,j}=Cov(\\mathbf{x}_i,\\mathbf{x}_j)$ 协方差的对角线元素给出了方差：$Cov(\\mathbf{x}_i,\\mathbf{x}_i)=Var(\\mathbf{x}_i)$。\n2.8 常见概率分布 在机器学习中有几个有用的概率分布。\n伯努利分布\n分布在单个二进制随机变量上。特性：\n$P(\\mathbf{x}=1)=\\phi$，$p(\\mathbf{x}=0)=1-\\phi$ $P(\\mathbf{x}=x)=\\phi^x(1-\\phi)^{1-x}$ $\\mathbb{E}_{\\mathbf{x}}[\\mathbf{x}]=\\phi$ $Var_\\mathbf{x}(\\mathbf{x})=\\phi(1-\\phi)$ 多项式分布\n或分类分布，是具有$k$个不同状态的单个离散变量的分布。\n高斯分布\n或正态分布： $\\mathcal{N}(x;\\mu,\\sigma^2)=\\sqrt{\\dfrac{1}{2\\pi \\sigma^2}}\\exp(-\\dfrac{1}{2\\sigma^2(x-\\mu)^2})$\n$\\mu$给出了中心峰值的坐标，这也是分布的均值：$\\mathbb{E}[\\mathbf{x}]=\\mu$ 分布的标准差：$\\sigma$ 方差：$\\sigma^2$ 指数和拉普拉斯分布\n指数分布：$p(x;\\lambda)=\\lambda 1_{x\\geq 0} \\exp(-\\lambda x)$\n对于所有负值的$x$，概率为零。\n拉普拉斯分布：$Laplace(x;\\mu,\\gamma)=\\dfrac{1}{2\\gamma}\\exp(-\\dfrac{|x-\\mu|}{\\gamma})$\n狄拉克分布和经验分布\n狄拉克分布：$p(x)=\\delta (x-\\mu)$\n经验分布：$\\hat{p}(x)=\\dfrac{1}{m}\\sum_{i=1}^m\\delta(x-x^{(i)})$\n2.9 常见函数的有用性质 Logistic sigmoid 函数\n$\\sigma(x)=\\dfrac{1}{1+\\exp(-x)}$\n它通常用于生成 Bermoulli 分布的 $\\phi$ 参数。当其参数非常正或负时，sigmoid 函数饱和，意味着函数变得非常平坦，对其输入的微小变化不敏感。\nSoftplus 函数\n$\\zeta(x)=\\log(1+\\exp(x))$\n该函数可用于生成正态分布的 $\\beta$ 或 $\\sigma$ 参数。\n重要性质\n$\\sigma(x)=\\dfrac{\\exp(x)}{\\exp(x)+1}$ $\\dfrac{d}{dx}\\sigma(x)=\\sigma(x)(1-\\sigma(x))$ $1-\\sigma(x)=\\sigma(-x)$ $\\log\\sigma(x) = -\\zeta(-x)$ $\\dfrac{d}{dx}\\zeta(x)=\\sigma (x)$ $\\forall x\\in (0,1), \\sigma^{-1}(x)=\\log(\\dfrac{x}{1-x})$ $\\forall x \u0026gt; 0, \\zeta^{-1}(x)=\\log (\\exp(x)-1)$ $\\zeta(x)=\\int_{-\\infin}^{x}\\sigma(y)dy$ $\\zeta (x) - \\zeta(-x) = x$ 2.10 贝叶斯定理 $P(x|y)=\\dfrac{P(x)P(y|x)}{P(y)}$\n通过 $P(y|x)$ 计算 $P(x|y)$，注意 $P(y)=\\sum_xP(y|x)P(x)$。贝叶斯定理是一种在拥有一些信息情况下计算某件事发生可能性的方法。\n3 应用问题 问题1：有一个公平的硬币（一面是正面，一面是反面）和一个不公平的硬币（两面都是反面）。你随机选择一个硬币，抛掷5次，观察到全部5次都是反面。你抛的是不公平的硬币的几率是多少？\n定义 $U$ 为抛出不公平硬币的情况；$F$ 表示抛出公平硬币。$5T$ 表示我们连续抛出5次正面的事件。\n我们知道 $P(U) = P(F) = 0.5$，需要求解 $P(U|5T)$。 $$P(U|5T) = \\dfrac{P(5T|U)P(U)}{P(5T)}$$ $$=\\dfrac{10.5}{P(5T|U)P(U)+P(5T|F)P(F)}$$ $$=\\dfrac{0.5}{10.5+0.5^5*0.5}\\approx0.97$$ 因此，选择了不公平硬币的概率约为97%。\n问题2：你和你的朋友正在玩一个游戏。你们两个将继续抛硬币，直到序列 HH 或 TH 出现为止。如果先出现 HH，你赢。如果先出现 TH，你的朋友赢。每个人的获胜概率是多少？\nP(HH 先出现而不是 TH) = P(前两次抛出 HH) = 1/4\nP(TH 先出现而不是 HH) = P(首次为 T) + P(前两次为 HT) = 1/2 + 1/4 = 3/4\n问题3：1000人中有1人患有一种特定的疾病，并且有一种检测方法，如果患有该疾病，检测正确率为98%。如果没有患病，检测错误率为1%。如果有人检测为阳性，他们患病的几率是多少？\nP(D) = 1/1000 表示患有疾病的概率\nP(H) = 1 - P(D) = 999/1000 表示健康的概率\nP(P|D) = 98% 表示如果患有疾病，则检测为阳性的概率\nP(P|H) = 1% 表示如果没有患病，则检测为阳性的概率\n需要求解 P(D|P)\n$$P(D|P)=\\dfrac{P(P|D)P(D)}{P(P)}$$ $$= \\dfrac{98/100*1/1000}{P(P|D)P(D) + P(P|H)P(H)}$$ $$= \\dfrac{0.098%}{98%*1/1000 + 1% * 999/1000}$$ $$\\approx 8.94%$$\n因此，如果有人检测为阳性，则他们患病的概率约为0.0894或8.94%。\n","link":"https://mengwoods.github.io/cn/post/dl/001-probability/","section":"post","tags":null,"title":"深度学习中的概率知识"},{"body":"应用案例\n使用 Scikit-learn 实现 PCA 使用Scikit-learn库执行PCA过程以对数字数据进行降维。然后比较原始数据和重构数据之间的差异。\n输入数据 导入库，并可视化源输入数据。\n1# 导入所需的库 2import numpy as np 3import matplotlib.pyplot as plt 4from sklearn.datasets import load_digits 5from sklearn.decomposition import PCA 6from sklearn.metrics import mean_squared_error 1# 加载数字数据集 2digits = load_digits() 3X = digits.data 4y = digits.target 5 6# 计算原始数据大小 7original_size = X.nbytes / (1024 * 1024) # in megabytes 8print(\u0026#34;original data size is: %.2f MB\u0026#34; % original_size) original data size is: 0.88 MB 1# 将前10个样本作为图像显示 2fig, axes = plt.subplots(1, 10, figsize=(12, 4)) 3for i in range(10): 4 axes[i].imshow(X[i].reshape(8, 8), cmap=\u0026#39;gray\u0026#39;) 5 axes[i].set_title(f\u0026#34;Label: {y[i]}\u0026#34;) 6 axes[i].axis(\u0026#39;off\u0026#39;) 7plt.tight_layout() 8plt.show() 重建结果 定义函数以计算重构误差和执行PCA\n1# Function to calculate reconstruction error 2def reconstruction_error(original, reconstructed): 3 return mean_squared_error(original, reconstructed) 4 5# 函数用于执行PCA并使用n_components重构数据。 6def perform_pca(n_components): 7 pca = PCA(n_components=n_components) 8 X_pca = pca.fit_transform(X) 9 X_reconstructed = pca.inverse_transform(X_pca) 10 return X_reconstructed, pca 1# 执行PCA，并可视化结果 2def analyze_pca(n_components): 3 X_reconstructed, pca = perform_pca(n_components) 4 reconstruction_error_val = reconstruction_error(X, X_reconstructed) 5 print(f\u0026#34;Number of Components: {n_components}, Reconstruction Error: {reconstruction_error_val}\u0026#34;) 6 7 # 压缩文件的大小 8 compressed_size = (pca.components_.nbytes + pca.mean_.nbytes + X_reconstructed.nbytes) / (1024 * 1024) # in megabytes 9 print(f\u0026#34;Size of Compressed File: {compressed_size} MB\u0026#34;) 10 11 # 大小差异 12 size_difference = original_size - compressed_size 13 print(f\u0026#34;Difference in Size: {size_difference} MB\u0026#34;) 14 15 # 绘制每个数字的原始和重构图像 16 fig, axes = plt.subplots(2, 10, figsize=(10, 2)) 17 for digit in range(10): 18 digit_indices = np.where(y == digit)[0] # Indices of samples with the current digit 19 original_matrix = X[digit_indices[0]].reshape(8, 8) # Take the first sample for each digit 20 reconstructed_matrix = np.round(X_reconstructed[digit_indices[0]].reshape(8, 8), 1) # Round to one decimal place 21 axes[0, digit].imshow(original_matrix, cmap=\u0026#39;gray\u0026#39;) 22 axes[0, digit].axis(\u0026#39;off\u0026#39;) 23 axes[1, digit].imshow(reconstructed_matrix, cmap=\u0026#39;gray\u0026#39;) 24 axes[1, digit].axis(\u0026#39;off\u0026#39;) 25 26 plt.suptitle(f\u0026#39;Reconstruction with {n_components} Components\u0026#39;) 27 plt.show() 28 29 # 打印第一个数据的原始矩阵 30 print(\u0026#34;Original Matrix of the First Data:\u0026#34;) 31 print(original_matrix) 32 33 # 打印重构矩阵 34 print(\u0026#34;\\nReconstruction Matrix of the First Data:\u0026#34;) 35 print(reconstructed_matrix) 分析使用一个主成分时的结果\n1analyze_pca(1) Number of Components: 1, Reconstruction Error: 15.977678462238496 Size of Compressed File: 0.87841796875 MB Difference in Size: -0.0009765625 MB Original Matrix of the First Data: [[ 0. 0. 11. 12. 0. 0. 0. 0.] [ 0. 2. 16. 16. 16. 13. 0. 0.] [ 0. 3. 16. 12. 10. 14. 0. 0.] [ 0. 1. 16. 1. 12. 15. 0. 0.] [ 0. 0. 13. 16. 9. 15. 2. 0.] [ 0. 0. 0. 3. 0. 9. 11. 0.] [ 0. 0. 0. 0. 9. 15. 4. 0.] [ 0. 0. 9. 12. 13. 3. 0. 0.]] Reconstruction Matrix of the First Data: [[-0. 0.4 6.4 12.6 12. 6.3 1.4 0.1] [ 0. 2.6 11.7 11.2 10.5 9.4 1.9 0.1] [ 0. 3. 9.4 5.8 8. 8.7 1.6 0. ] [ 0. 2.1 7.7 9. 11.1 7.8 2. 0. ] [ 0. 1.5 5.6 8.2 9.8 8.5 2.8 0. ] [ 0. 1. 5.2 5.9 6.5 8.2 3.7 0. ] [ 0. 0.8 7.8 9. 8.8 9.5 4.1 0.2] [ 0. 0.4 6.8 12.9 11.9 7.3 2.3 0.4]] 分析使用五个主成分时的结果\n1analyze_pca(5) Number of Components: 5, Reconstruction Error: 8.542447616249266 Size of Compressed File: 0.88037109375 MB Difference in Size: -0.0029296875 MB Original Matrix of the First Data: [[ 0. 0. 11. 12. 0. 0. 0. 0.] [ 0. 2. 16. 16. 16. 13. 0. 0.] [ 0. 3. 16. 12. 10. 14. 0. 0.] [ 0. 1. 16. 1. 12. 15. 0. 0.] [ 0. 0. 13. 16. 9. 15. 2. 0.] [ 0. 0. 0. 3. 0. 9. 11. 0.] [ 0. 0. 0. 0. 9. 15. 4. 0.] [ 0. 0. 9. 12. 13. 3. 0. 0.]] Reconstruction Matrix of the First Data: [[ 0. 0.2 5.2 11.1 12.1 7. 1.6 0.1] [ 0. 2.1 11.2 10.7 9.7 9.6 2.3 0.2] [ 0. 3.1 11.2 6.2 6. 9.2 2.5 0.1] [ 0. 3.1 10.3 9. 9.6 9.6 2.9 0. ] [ 0. 2.2 6. 5.3 8. 11.6 3.9 0. ] [ 0. 1.2 4.2 1.9 4.9 11.7 5.1 0. ] [ 0. 0.6 6.7 6.2 8.8 12.1 4.4 0.2] [ 0. 0.2 5.4 12.1 13.4 8.2 1.8 0.3]] 使用更多的主成分，重构结果会更好。接下来我们将手动计算PCA矩阵。\n手动实现 PCA 手动逐步执行PCA分析。\n输入数据 打印数据\n1# 然后使用逐步的方法计算PCA步骤； 2# 取第一个数据点进行分析 3first_data = X[0] 4print(\u0026#34;Raw input data: \\n\u0026#34;, X[0]) 5# 将数据点重新整形为二维数组（图像） 6input_matrix = first_data.reshape(8, 8) 7 8print(\u0026#34;Input matrix: \u0026#34;) 9for row in input_matrix: 10 print(\u0026#34; \u0026#34;.join(f\u0026#34;{val:4.0f}\u0026#34; for val in row)) 11 12# 打印原始矩阵（图像） 13plt.imshow(input_matrix, cmap=\u0026#39;gray\u0026#39;) 14plt.title(\u0026#34;Input matrix (Image)\u0026#34;) 15plt.axis(\u0026#39;off\u0026#39;) 16plt.show() Raw input data: [ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5. 0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8. 8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1. 12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13. 10. 0. 0. 0.] Raw data shape: (64,) Input matrix: 0 0 5 13 9 1 0 0 0 0 13 15 10 15 5 0 0 3 15 2 0 11 8 0 0 4 12 0 0 8 8 0 0 5 8 0 0 9 8 0 0 4 11 0 1 12 7 0 0 2 14 5 10 12 0 0 0 0 6 13 10 0 0 0 居中数据 这个均值计算有助于我们理解每个特征的平均值，这对于居中数据和在随后的步骤中计算协方差矩阵至关重要。 居中数据是PCA中的关键预处理步骤，它增强了结果的解释性，消除了偏差，并确保了计算中的数值稳定性。\n1# 步骤1：计算每个特征（列）的均值 2mean_vec = np.mean(input_matrix, axis=0) 3print(mean_vec) [ 0. 2.25 10.5 6. 5. 8.5 4.5 0. ] 1# 步骤2：从每个特征中减去均值 2centered_matrix = input_matrix - mean_vec 3print(centered_matrix) [[ 0. -2.25 -5.5 7. 4. -7.5 -4.5 0. ] [ 0. -2.25 2.5 9. 5. 6.5 0.5 0. ] [ 0. 0.75 4.5 -4. -5. 2.5 3.5 0. ] [ 0. 1.75 1.5 -6. -5. -0.5 3.5 0. ] [ 0. 2.75 -2.5 -6. -5. 0.5 3.5 0. ] [ 0. 1.75 0.5 -6. -4. 3.5 2.5 0. ] [ 0. -0.25 3.5 -1. 5. 3.5 -4.5 0. ] [ 0. -2.25 -4.5 7. 5. -8.5 -4.5 0. ]] 协方差计算 计算居中数据的协方差矩阵。\n1# 使用np.dot计算协方差。Bessel修正在末尾减1。 https://www.uio.no/studier/emner/matnat/math/MAT4010/data/forelesningsnotater/bessel-s-correction---wikipedia.pdf 2cov_matrix = np.dot(centered_matrix.T, centered_matrix) / (centered_matrix.shape[0] - 1) 3 4# 或者使用np.cov计算协方差 5# cov_matrix = np.cov(centered_matrix, rowvar=False) 6 7print(cov_matrix) [[ 0. 0. 0. 0. 0. 0. 0. 0. ] [ 0. 4.21428571 2.28571429 -13.14285714 -9.42857143 4.14285714 6.14285714 0. ] [ 0. 2.28571429 14. -9.42857143 -4.85714286 17. 6.28571429 0. ] [ 0. -13.14285714 -9.42857143 43.42857143 29.57142857 -12.57142857 -17.85714286 0. ] [ 0. -9.42857143 -4.85714286 29.57142857 26. -7. -17.57142857 0. ] [ 0. 4.14285714 17. -12.57142857 -7. 28.85714286 11. 0. ] [ 0. 6.14285714 6.28571429 -17.85714286 -17.57142857 11. 14.85714286 0. ] [ 0. 0. 0. 0. 0. 0. 0. 0. ]] 矩阵特征分解 1# 步骤4：计算协方差矩阵的特征值和特征向量 2eigenvalues, eigenvectors = np.linalg.eig(cov_matrix) 3 4print(eigenvalues) 5print(eigenvectors) [8.92158455e+01 3.14545089e+01 7.61850164e+00 2.85144338e+00 2.01453633e-01 1.53898738e-02 0.00000000e+00 0.00000000e+00] [[ 0. 0. 0. 0. 0. 0. 1. 0. ] [-0.20365153 0.09344175 0.07506402 -0.23052329 -0.41043409 -0.85003703 0. 0. ] [-0.22550077 -0.48188982 0.20855091 0.79993174 -0.1168451 -0.14104805 0. 0. ] [ 0.65318552 -0.28875672 -0.59464342 0.12374602 0.11324705 -0.32898247 0. 0. ] [ 0.48997693 -0.31860576 0.39448425 -0.20610464 -0.63307453 0.24399318 0. 0. ] [-0.33563583 -0.75773097 -0.0607778 -0.49775699 0.24837474 0.00681139 0. 0. ] [-0.35818338 -0.00212894 -0.66178497 0.03760326 -0.58531429 0.29955628 0. 0. ] [ 0. 0. 0. 0. 0. 0. 0. 1. ]] 选择与最大特征值对应的特征向量作为主成分。\n1# 步骤5：选择与最大特征值对应的主成分 2max_eigenvalue_index = np.argmax(eigenvalues) 3principal_component = eigenvectors[:, max_eigenvalue_index] 4print(principal_component) [ 0. -0.20365153 -0.22550077 0.65318552 0.48997693 -0.33563583 -0.35818338 0. ] 1# 步骤6：将数据投影到主成分上 2reduced_data = np.dot(centered_matrix, principal_component) 3 4# 打印降维后的数据 5print(\u0026#34;Reduced data (1 principal component):\\n\u0026#34;, reduced_data) Reduced data (1 principal component): [12.35977044 5.86229378 -8.32285024 -8.14946302 -7.7867473 -8.41834525 1.49545914 12.95988243] 到目前为止，数据已从 8x8 矩阵压缩为 8x1 向量。\n重构数据 现在基于降维后的结果来重新组建原数据，并展示效果。\n1# 步骤7：重构数据 2reconstructed_data = np.dot(reduced_data.reshape(-1, 1), principal_component.reshape(1, -1)) 3 4# 步骤8：将均值添加回重构的数据 5reconstructed_data += mean_vec 6 7# 步骤9：可视化原始数据和重构数据 8fig, axes = plt.subplots(1, 2, figsize=(12, 6)) 9 10# 原始数据 11axes[0].imshow(input_matrix, cmap=\u0026#39;gray\u0026#39;) 12axes[0].set_title(\u0026#39;Original Data\u0026#39;) 13axes[0].axis(\u0026#39;off\u0026#39;) 14 15# Reconstructed data 16axes[1].imshow(reconstructed_data.real, cmap=\u0026#39;gray\u0026#39;) 17axes[1].set_title(\u0026#39;Reconstructed Data\u0026#39;) 18axes[1].axis(\u0026#39;off\u0026#39;) 19 20plt.show() ","link":"https://mengwoods.github.io/cn/post/math/003-pca-application/","section":"post","tags":null,"title":"主成分分析 (PCA) 的应用案例"},{"body":"Based on Deep Learning (2017, MIT) book.\n本文基于Deep Learning (2017, MIT)，推导过程补全了所涉及的知识及书中推导过程中跳跃和省略的部分。 blog\n1 概述 现代数据集，如网络索引、高分辨率图像、气象学、实验测量等，通常包含高维特征，高纬度的数据可能不清晰、冗余，甚至具有误导性。数据可视化和解释变量之间的关系很困难，而使用这种高维数据训练的神经网络模型往往容易出现过拟合（维度诅咒）。 主成分分析（PCA）是一种简单而强大的无监督机器学习技术，用于数据降维。它旨在从大型变量集中提取一个较小的数据集，同时尽可能保留原始信息和特征（有损压缩）。PCA有助于识别数据集中最显著和有意义的特征，使数据易于可视化。应用场景包括：统计学、去噪和为机器学习算法预处理数据。\n主成分是什么？ 主成分是构建为原始变量的线性组合的新变量。这些新变量是不相关的，并且包含原始数据中大部分的信息。 2 背景数学知识 这些知识对下一节的推导很重要。\n正交向量和矩阵： 如果两个向量垂直，则它们是正交的。即两个向量的点积为零。 正交矩阵是一个方阵，其行和列是相互正交的单位向量；每两行和两列的点积为零，每一行和每一列的大小为1。 如果$A^T=A^{-1}$或$AA^T=A^TA=I$，则$A$是正交矩阵。 在机器人学中，旋转矩阵通常是一个$3\\times3$的正交矩阵，在空间变换中它会旋转向量的方向但保持原始向量的大小。 矩阵、向量乘法规则： $(AB)^T=B^TA^T$，两个矩阵的乘积的转置。 $\\vec{a}^T\\vec{b}=\\vec{b}^T\\vec{a}$，两个结果都是标量，标量的转置是相同的。 $(A + B)C = AC + BC$，乘法是可分配的。 $AB \\neq{} BA$，乘法一般不满足交换律。 $A(BC)=(AB)C$，乘法满足结合律。 对称矩阵： $A=A^T$，$A$是对称矩阵。 $X^TX$是对称矩阵，因为$(X^TX)^T=X^TX$。 向量导数规则（$B$是常量矩阵）： $d(x^TB)/dx=B$ $d(x^Tx)/dx=2x$ $d(x^TBx)/dx=2Bx$ 矩阵迹规则： $Tr(A)=Tr(A^T)$ $Tr(AB)=Tr(BA)$ $Tr(A)=\\sum_i{\\lambda_i}$，其中$\\lambda$是$A$的特征值。 迹在循环移位下不变：$Tr(ABCD)=Tr(BCDA)=Tr(CDAB)=Tr(DABC)$ 向量和矩阵范数： 向量的$L^2$范数，也称为欧几里得范数：$||x||_2=\\sqrt{\\sum_i|x_i|^2}$。 通常使用平方的$L^2$范数来衡量向量的大小，可以计算为$x^Tx$。 Frobenius范数用于衡量矩阵的大小：$||A||F=\\sqrt{\\sum{i,j}A^2_{i,j}}$ Frobenius范数是所有矩阵元素的绝对平方和的平方根。 Frobenius范数是矩阵版本的欧几里得范数。 特征值分解和特征值： 方阵$A$的特征向量是一个非零向量$v$，使得$A$的乘法仅改变$v$的比例：$Av=\\lambda v$。$\\lambda$是特征值，$v$是特征向量。 假设矩阵$A$有$n$个线性无关的特征向量$v^{(i)}$，我们可以将所有特征向量连接起来形成一个矩阵$V=[v^{(1)},\\ldots,v^{(n)}]$，并通过连接所有特征值$\\lambda=[\\lambda_1,\\ldots,\\lambda_n]^T$形成一个向量，那么$A$的特征分解是$A=Vdiag(\\lambda)V^{-1}$ 每个实对称矩阵都可以分解为$A=Q\\Lambda Q^T$，其中$Q$是由$A$的特征向量组成的正交矩阵，$\\Lambda$（读作'lambda'）是一个对角矩阵。 拉格朗日乘数法： 拉格朗日乘数法是一种在方程约束下寻找函数局部最大值和最小值的策略。 一般形式：$\\mathcal{L}(x,\\lambda)=f(x)+\\lambda\\cdot g(x)$，$\\lambda$称为拉格朗日乘子。 3 详细PCA推导 需求描述\n我们有$m$个点的输入数据，表示为${x^{(1)},...,x^{(m)}}$在$\\mathbb{R}^{n}$的实数集中。因此，每个点$x^{(i)}$是一个列向量，具有$n$维特征。\n需要对输入数据进行有损压缩，将这些点编码以表示它们的较低维度版本。换句话说，我们想要找到编码向量$c^{(i)}\\in \\mathbb{R}^{l}$，$(l\u0026lt;n)$来表示每个输入点$x^{(i)}$。我们的目标是找到产生输入的编码向量的编码函数$f(x)=c$，以及相应的重构（解码）函数$x\\approx g(f(x))$，根据编码向量$c$计算原始输入。\n解码的$g(f(x))$是一组新的点（变量），因此它与原始$x$是近似的。存储$c^{(i)}$和解码函数比存储$x^{(i)}$更节省空间，因为$c^{(i)}$的维度较低。\n解码矩阵\n我们选择使用矩阵$D$作为解码矩阵，将编码向量$c^{(i)}$映射回$\\mathbb{R}^{n}$，因此$g(c)=Dc$，其中$D\\in \\mathbb{R}^{n\\times l}$。为了简化编码问题，PCA将$D$的列约束为彼此正交。\n衡量重构的表现\n在继续之前，我们需要弄清楚如何生成最优的编码点$c^{}$，我们可以测量输入点$x$与其重构$g(c^)$之间的距离，使用$L^2$范数（或欧几里得范数）：$c^{*}=\\arg\\min_c||x-g(c)||_2$。由于$L^2$范数是非负的，并且平方操作是单调递增的，所以我们可以转而使用平方的$L^2$范数：\n$$c^{*}={\\arg\\min}_c||x-g(c)||_2^2$$ 向量的$L^2$范数是其分量的平方和，它等于向量与自身的点积，例如$||x||_2=\\sqrt{\\sum|x_i|^2}=\\sqrt{x^Tx}$，因此平方的$L^2$范数可以写成以下形式：\n$$||x-g(c)||_2^2 = (x-g(c))^T(x-g(c))$$ 由分配率： $$=(x^T-g(c)^T)(x-g(c))=x^Tx-x^Tg(c)-g(c)^Tx+g(c)^Tg(c)$$ 由于$x^Tg(c)$和$g(c)^Tx$是标量，标量等于其转置，$(g(c)^Tx)^T=x^Tg(c)$，所以： $$=x^Tx-2x^Tg(c)+g(c)^Tg(c)$$ 为了找到使上述函数最小化的$c$，第一项可以省略，因为它不依赖于$c$，所以： $$c^={\\arg\\min}_c-2x^Tg(c)+g(c)^Tg(c)$$ 然后用$g(c)$的定义$Dc$进行替换： $$={\\arg\\min}_c-2x^TDc+c^TD^TDc$$ 由于$D$的正交性和单位范数约束： $$c^={\\arg\\min}_c-2x^TDc+c^TI_lc$$ $$= {\\arg\\min}_c-2x^TDc+c^Tc$$\n目标函数\n现在目标函数是$-2x^TDc+c^Tc$，我们需要找到$c^*$来最小化目标函数。使用向量微积分，并令其导数等于0： $$\\nabla_c(-2x^TDc+c^Tc)=0$$ 根据向量导数规则： $$-2D^Tx+2c=0 \\Rightarrow c=D^Tx$$\n找到编码矩阵 $D$\n所以编码器函数是 $f(x)=D^Tx$。因此我们可以定义 PCA 重构操作为 $r(x)=g(f(x))=D(D^Tx)=DD^Tx$。\n因此编码矩阵 $D$ 也被重构过程使用。我们需要找到最优的 $D$ 来最小化重构误差，即输入和重构之间所有维度特征的距离。这里使用 Frobenius 范数（矩阵范数）定义目标函数： $$D^={\\arg\\min}D\\sqrt{\\sum{i,j}(x_j^{(i)}-r(x^{i})_j)^2},\\quad D^TD=I_l$$\n从考虑 $l=1$ 的情况开始（这也是第一个主成分），$D$ 是一个单一向量 $d$，并使用平方 $L^2$ 范数形式： $$d^={\\arg\\min}d{\\sum{i}||(x^{(i)}-r(x^{i}))}||_2^2, ||d||_2=1$$ $$ = {\\arg\\min}d{\\sum{i}||(x^{(i)}-dd^Tx^{(i)})||_2^2}, ||d||_2=1$$ $d^Tx^{(i)}$ 是一个标量： $$= {\\arg\\min}d{\\sum{i}||(x^{(i)}-d^Tx^{(i)}d)}||_2^2, ||d||_2=1$$\n标量等于其自身的转置：\n$$d^*= {\\arg\\min}d{\\sum{i}||(x^{(i)}-x^{(i)T}dd)}||_2^2, ||d||_2=1$$\n使用矩阵形式表示\n令 $X\\in \\mathbb{R}^{m\\times n}$ 表示所有描述点的向量堆叠，即 ${x^{(1)^T}, x^{(2)^T}, \\ldots, x^{(i)^T}, \\ldots, x^{(m)^T}}$，使得 $X_{i,:}=x^{(i)^T}$。\n$$ X = \\begin{bmatrix} x^{(1)^T}\\ x^{(2)^T}\\ \\ldots\\ x^{(m)^T} \\end{bmatrix} \\Rightarrow Xd = \\begin{bmatrix} x^{(1)^T}d\\ x^{(2)^T}d\\ \\ldots\\ x^{(m)^T}d \\end{bmatrix} $$\n$$ \\Rightarrow Xdd^T = \\begin{bmatrix} x^{(1)^T}dd^T\\ x^{(2)^T}dd^T\\ \\ldots\\ x^{(m)^T}dd^T\\ \\end{bmatrix} $$\n$$ \\Rightarrow X-Xdd^T = \\begin{bmatrix} x^{(1)^T}-x^{(1)^T}dd^T\\ x^{(2)^T}-x^{(2)^T}dd^T\\ \\ldots\\ x^{(m)^T}-x^{(m)^T}dd^T\\ \\end{bmatrix} $$\n矩阵中的一行的转置： $$(x^{(i)^T}-x^{(i)^T}dd^T)^T=x^{(i)}-dd^Tx^{(i)}$$ 由于 $d^Tx^{(i)}$ 是标量： $$=x^{(i)}-d^Tx^{(i)}d=x^{(i)}-x^{(i)^T}dd$$ 所以我们知道 $X$ 的第 $i$ 行的 $L^2$ 范数与原始形式相同，因此我们可以使用矩阵重写问题，并省略求和符号： $$d^={\\arg\\min}{d}||X-Xdd^T||F^2, \\quad d^Td=1 $$ 利用矩阵迹规则简化 Frobenius 范数部分如下： $${\\arg\\min}{d}||X-Xdd^T||F^2$$ $$={\\arg\\min}{d}Tr((X-Xdd^T)^T(X-Xdd^T))$$ $$={\\arg\\min}{d}-Tr(X^TXdd^T)-Tr(dd^TX^TX)+Tr(dd^TX^TXdd^T)$$ $$={\\arg\\min}{d}-2Tr(X^TXdd^T)+Tr(X^TXdd^Tdd^T)$$ 由于 $d^Td=1$： $$={\\arg\\min}{d}-2Tr(X^TXdd^T)+Tr(X^TXdd^T)$$ $$={\\arg\\min}{d}-Tr(X^TXdd^T)$$ $$={\\arg\\max}{d}Tr(X^TXdd^T)$$ 由于迹是循环置换不变的，将方程重写为： $$d^={\\arg\\max}{d}Tr(d^TX^TXd), \\quad d^Td=1$$ 由于 $d^TX^TXd$ 是实数，因此迹符号可以省略： $$d^*={\\arg\\max}{d}d^TX^TXd,\\quad d^Td=1$$\n寻找最优的 $d$\n现在的问题是找到最优的 $d$ 来最大化 $d^TX^TXd$，并且有约束条件 $d^Td=1$。\n使用拉格朗日乘子法来将问题描述为关于 $d$ 的形式： $$\\mathcal{L}(d,\\lambda)=d^TX^TXd+\\lambda(d^Td-1)$$\n对 $d$ 求导数（向量导数规则）： $$\\nabla_d\\mathcal{L}(d,\\lambda)=2X^TXd+2\\lambda d$$\n令导数等于0，$d$ 将是最优的： $$2X^TXd+2\\lambda d=0$$ $$X^TXd=-\\lambda d$$ $$X^TXd=\\lambda' d,\\quad(\\lambda'=-\\lambda)$$\n这个方程是典型的矩阵特征值分解形式，$d$ 是矩阵 $X^TX$ 的特征向量，$\\lambda'$ 是对应的特征值。\n利用上述结果，让我们重新审视原方程： $$d^*={\\arg\\max}{d}d^TX^TXd, \\quad d^Td=1$$ $$={\\arg\\max}{d}d^T\\lambda' d$$ $$={\\arg\\max}{d}\\lambda'd^T d$$ $$={\\arg\\max}{d}\\lambda'$$\n现在问题已经变的非常清楚了，$X^TX$ 的最大特征值会最大化原方程的结果，因此最优的 $d$ 是矩阵 $X^TX$ 对应最大特征值的特征向量。\n这个推导是针对 $l=1$ 的情况，只包含第一个主成分。当 $l\u0026gt;1$ 时，$D=[d_1, d_2, \\ldots]$，第一个主成分 $d_1$ 是矩阵 $X^TX$ 对应最大特征值的特征向量，第二个主成分 $d_2$ 是对应第二大特征值的特征向量，以此类推。\n4 总结 我们有一个数据集，包含 $m$ 个点，记为 ${x^{(1)},...,x^{(m)}}$。 令 $X\\in \\mathbb{R}^{m\\times n}$ 为将所有这些点堆叠而成的矩阵：$[x^{(1)^T}, x^{(2)^T}, \\ldots, x^{(i)^T}, \\ldots, x^{(m)^T}]$。\n主成分分析（PCA）编码函数表示为 $f(x)=D^Tx$，重构函数表示为 $x\\approx g(c)=Dc$，其中 $D=[d_1, d_2, \\ldots]$ 的列是 $X^TX$ 的特征向量，特征向量对应的特征值大小为降序排列。$D^Tx$即是降维度之后的数据。\n后续分析PCA的应用案例文章\n","link":"https://mengwoods.github.io/cn/post/math/003-pca/","section":"post","tags":null,"title":"主成分分析(PCA)的详细推导过程"},{"body":"介绍一些我常用的C++容器和使用方法，以及使用案例。\n1 概述 容器（Container）是一个存储其他对象集合的持有者对象。容器以类模板实现，对支持的元素类型有很大的灵活性。容器管理元素的存储并提供多个成员函数来访问和操作元素。\n两个主要类别：\n序列容器（Sequence container）：将元素维护在线性序列中，通过元素的位置来访问索引某个元素，可以用来存放不需要检索的数据。 关联容器（Associative container）：允许基于键（key）而不是位置进行有效检索（即搜索某个key获取其内容）。通常使用二叉搜索树或哈希表实现。可以用来放置频繁检索的数据。 下面，我将介绍几种常用的序列容器和无序关联容器以及它们的典型用例。\n2 序列容器 Sequence Container 序列容器指的是标准库（STL）中实现数据元素存储的一组容器类模板。包括Array、Vector、List、Forward List、Deque。\nArray：编译时大小不可调整的容器类型。 Vector：具有快速随机访问并在添加元素时自动调整大小的容器类型。 Deque：具有相对较快的随机访问的双端队列。 List：双向链表。 Forward list：单向链表。 2.1 Array std::array 是封装固定大小数组的容器。\n1#include \u0026lt;array\u0026gt; 2// 声明一个包含 3 个元素的整数array 3array\u0026lt;int, 3\u0026gt; arr = {0, 1, 2}; 4// 使用 size() 成员函数获取array的大小 5arr.size(); 6// 使用下标运算符 [] 访问array中索引为 0 的元素 7arr[0]; 2.2 Vector std::vector 是 C++ 标准库中的类模板。它表示可以改变容量大小的序列容器，使用连续的存储位置存储元素。Vector的大小可以动态改变，容器自动处理存储。它动态分配数组以存储元素，与Array相比，Vector消耗更多的内存来管理存储内容。\n1#include \u0026lt;vector\u0026gt; 2// 声明和初始化一个 vector 的方式 3vector\u0026lt;int\u0026gt; vec = {1, 2, 4}; 4vector\u0026lt;int\u0026gt; vec {1, 2, 3}; 5vector\u0026lt;int\u0026gt; vec(4, 3); // 4 是大小，3 是值 {3, 3，3， 3} 6// 将索引为 3 的元素赋值为 5 7vec[3] = 5; 8// 声明一个字符串向量 9vector\u0026lt;string\u0026gt; planets; 10// 向向量中添加字符串 \u0026#34;Mercury\u0026#34; 11planets.push_back(\u0026#34;Mercury\u0026#34;); 12// 检索向量中的元素数量 13planets.size(); 14// 检索向量当前的容量 15planets.capacity(); 3 无序关联容器 Unordered Associative Container 关联容器存储由键值（key）和映射值（map）组合形成的元素。基于键快速检索单个元素。键值通常用于唯一标识元素。\n3.1 无序映射 Unordered Map 无序映射是 C++ 中高效存储键值对而不维护特定顺序的关联容器。它们提供基于键的快速检索、插入和删除操作，适用于需要快速访问由键标识的元素的场景。与有序容器相比，无序映射优先速度而不是元素顺序，提供更快的元素访问。\n1#include \u0026lt;unordered_map\u0026gt; 2// 声明一个具有整数键和值的无序映射 3std::unordered_map\u0026lt;int, int\u0026gt; freq_counter; 4// 访问与键 1 关联的值 5freq_counter[1]; 6// 将一个键值对插入到无序映射中 7freq_counter.insert(std::make_pair(2, 1)); 3.2 无序集合 Unordered Set 无序集合是以无序方式存储一组唯一元素的容器。无序集合不维护其元素之间的特定顺序。它们提供快速的检索、插入和删除操作，通常使用哈希表实现。这使它们适用于需要快速查找唯一元素的场景，而不用担心顺序。\n1#include \u0026lt;unordered_set\u0026gt; 2// 声明并初始化一个包含整数元素的无序集合 3std::unordered_set\u0026lt;int\u0026gt; mySet{2, 7, 1, 8, 2, 8}; 4// 向无序集合中插入值 5 5mySet.insert(5); 6// 如果存在，从无序集合中删除值 5 7mySet.erase(5); 3.3 应用场景 需求描述\n工作中遇到的一个使用案例： 设计一个目标车辆速度管理系统，旨在存储和调节交通车辆（障碍物）的速度。如果检测到车辆的速度大小不确定，系统将检索并应用上次该目标已知的速度大小以及其当前的速度方向。目的是维持交通上目标车辆的速度幅度稳定程度，尽量减少速度的突然变化，来弥补感知系统中Tracker自身的不足，因为如果某个车辆突然转向或突然出现到场景中，其速度可信程度不高。\n代码片段示例\n1// 初始化用于存储障碍物对象和障碍物 ID 的关联容器 2std::unordered_map\u0026lt;int, Eigen::Vector3d\u0026gt; obstacles_; 3std::unordered_set\u0026lt;int\u0026gt; obstacle_ids_; 4 5// 添加障碍物信息 6obstacle_ids_.insert(id); 7obstacles_[id] = velocity; 8 9// 从容器中移除障碍物 10obstacles_.erase(id); 11obstacle_ids_.erase(id); 12 13// 获取最后的速度 14auto it = obstacles_.find(id); 15// 如果找到 ID，则返回速度 16if (it != obstacles_.end()) 17{ 18 return it-\u0026gt;second; 19} 20else 21{ 22 // 如果未找到，则返回零速度 23} 24 25// 移除不再需要的障碍物 ID 26std::unordered_set\u0026lt;int\u0026gt; ids_to_remove; 27for (const auto\u0026amp; obstacle : obstacles_) 28{ 29 int id = obstacle.first; 30 if (obstacle_ids_.find(id) == obstacle_ids_.end()) 31 { 32 ids_to_remove.insert(id); 33 } 34} 35for (int id_to_remove : ids_to_remove) 36{ 37 obstacles_.erase(id_to_remove); 38} 39 40// 清除信息 41obstacle_ids_.clear(); 42 43// 使用最后速度的大小以及当前方向 44double magnitude = last_velocity.norm(); 45// 将当前速度归一化以保持其方向 46if (current_velocity.norm() \u0026gt; 0.0) // 避免除以零 47{ 48 current_velocity.normalize(); 49} 50else 51{ 52 // 如果当前速度为零，直接返回它 53 return current_velocity; 54} 55// 将归一化后的当前速度按照最后速度的大小进行缩放 56current_velocity *= magnitude; 57// 得到所需的速度 58return current_velocity; 4 LeetCode 题 3005 Count Elements With Maximum Frequency\nYou are given an array nums consisting of positive integers. Return the total frequencies of elements in nums such that those elements all have the maximum frequency. The frequency of an element is the number of occurrences of that element in the array.\nExample 1:\nInput: nums = [1,2,2,3,1,4] Output: 4 Explanation: The elements 1 and 2 have a frequency of 2 which is the maximum frequency in the array. So the number of elements in the array with maximum frequency is 4.\nExample 2:\nInput: nums = [1,2,3,4,5] Output: 5 Explanation: All elements of the array have a frequency of 1 which is the maximum. So the number of elements in the array with maximum frequency is 5. Constraints: 1 \u0026lt;= nums.length \u0026lt;= 100 1 \u0026lt;= nums[i] \u0026lt;= 100\n4.1 使用 Vector code file\n1#include \u0026lt;iostream\u0026gt; 2#include \u0026lt;vector\u0026gt; 3 4using namespace std; 5 6class Solution 7{ 8public: 9 int maxFrequencyElements(vector\u0026lt;int\u0026gt;\u0026amp; nums) 10 { 11 vector\u0026lt;int\u0026gt; frequency (nums.size(), 0); 12 for (int i = 0; i \u0026lt; frequency.size(); i ++) 13 { 14 frequency[i] = countDuplicatedNumber(i, nums); 15 } 16 for (int element : frequency) 17 { 18 std::cout \u0026lt;\u0026lt; element \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 19 } 20 cout \u0026lt;\u0026lt; endl; 21 int max_value = checkMaxValue(frequency); 22 return sumMaxValue(max_value, frequency); 23 } 24 int countDuplicatedNumber(const int\u0026amp; index, const vector\u0026lt;int\u0026gt;\u0026amp; vector) 25 { 26 int number = 1; 27 for (int i = 1; i + index \u0026lt; vector.size(); i++) 28 { 29 if (vector[i + index] == vector[index]) 30 { 31 number ++; 32 } 33 } 34 return number; 35 } 36 int checkMaxValue(const vector\u0026lt;int\u0026gt;\u0026amp; vector) 37 { 38 int max = 0; 39 for (int i = 0; i \u0026lt; vector.size(); i++) 40 { 41 if (vector[i] \u0026gt; max) 42 { 43 max = vector[i]; 44 } 45 } 46 cout \u0026lt;\u0026lt; \u0026#34;max value in the vec is: \u0026#34; \u0026lt;\u0026lt; max \u0026lt;\u0026lt; endl; 47 return max; 48 } 49 int sumMaxValue(const int\u0026amp; max, const vector\u0026lt;int\u0026gt;\u0026amp; vector) 50 { 51 int sum = 0; 52 for (int i = 0; i \u0026lt; vector.size(); i++) 53 { 54 if (vector[i] == max) 55 { 56 sum += vector[i]; 57 } 58 } 59 return sum; 60 } 61}; 62 63int main() 64{ 65 vector\u0026lt;int\u0026gt; num1 {1,2,2,3,4,4,1}; 66 Solution solution; 67 float result = solution.maxFrequencyElements(num1); 68 cout \u0026lt;\u0026lt; \u0026#34;result is: \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; endl; 69} 4.1 使用 Unnordered map code file, [reference]\n1#include \u0026lt;iostream\u0026gt; 2#include \u0026lt;vector\u0026gt; 3#include \u0026lt;unordered_map\u0026gt; 4 5using namespace std; 6 7class Solution 8{ 9public: 10 int maxFrequencyElements(vector\u0026lt;int\u0026gt;\u0026amp; nums) 11 { 12 std::unordered_map\u0026lt;int, int\u0026gt; freq_counter; 13 for(int num : nums) 14 { 15 freq_counter[num]++; 16 } 17 18 int max_frequency = 0; 19 for (const auto\u0026amp; entry : freq_counter) 20 { 21 max_frequency = std::max(max_frequency, entry.second); 22 } 23 24 int max_freq_elements = 0; 25 for (const auto\u0026amp; entry : freq_counter) 26 { 27 if (entry.second == max_frequency) 28 { 29 max_freq_elements++; 30 } 31 } 32 33 int total_frequency = max_frequency * max_freq_elements; 34 return total_frequency; 35 } 36}; 37 38int main() 39{ 40 vector\u0026lt;int\u0026gt; num1 {7,7,7,1,2,2,3,4,4,1}; 41 Solution solution; 42 int result = solution.maxFrequencyElements(num1); 43 cout \u0026lt;\u0026lt; \u0026#34;result is: \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; endl; 44} ","link":"https://mengwoods.github.io/cn/post/code/002-c++-containers/","section":"post","tags":null,"title":"C++ 常用容器以及一些应用案例"},{"body":"向量点乘，叉乘的概念，以及他们的应用及相关C++代码的实现。\n1 向量 向量具有大小和方向。 共线向量：两个平行的向量为共线向量。\n1.1 叉积 Cross Product $$\\vec{a}\\times\\vec{b}=|\\vec{a}||\\vec{b}|\\sin{\\theta}\\vec{n}$$\n$\\theta$是两个向量之间的角度，$\\vec{n}$是与两个向量都垂直的单位向量，方向遵循右手定则（右手食指从$\\vec{a}$划到$\\vec{b}$，大拇指的方向）。\n两个向量的叉积结果是一个与两者都垂直的向量。叉积的幅度值大小等于由这两个向量为边组成的平行四边形的面积。当两个向量垂直时，大小也达到最大，及矩形的面积。（这个特性决定了他可以用来计算空间中一个点到一个直线的距离，利用几何中平行四边形的面积同时等于底乘高，后面会介绍。）\n三维空间中，叉积的结果也可以用3x3矩阵的行列式表示。\n$$ \\vec{a}\\times \\vec{b}=\\det(\\vec{i},\\vec{j},\\vec{k};a_1,a_2,a_3;b_1,b_2,b_3)\\ =(a_2b_3 - a_3,b_2)\\vec{i}+(a_3b_1 - a_1,b_3)\\vec{j}+(a_1b_2 - a_2,b_1)\\vec{k}$$\n1.2 点积 Dot Product 叉积给出一个向量结果，但点积给出一个标量结果。 它将向量的相同方向投影的的长度相乘，因此使用$\\cos{\\theta}$将其中一个向量投影到另一个上。所以如果两个向量成直角，那么结果为零。点积更容易理解一些。\n$$ \\vec{a}\\cdot \\vec{b} = |\\vec{a}||\\vec{b}|\\cos{\\theta} $$\n2 实际应用 判断两个向量是否：\n共线：$\\vec{A}$=k*$\\vec{B}$，其中k是一个标量；叉积是零向量（仅适用于三维空间）；对应坐标的比率相等。 垂直：点积为零。 计算点P在线段AB上的投影点C坐标。\n向量$\\vec{AB}$，$\\vec{AP}$，点积结果为D。$\\vec{AB} \\cdot \\vec{AP}=D$ 推导一下：$\\vec{AB} \\cdot \\vec{AP}=|\\vec{AB}| |\\vec{AP}| \\cos{\\theta}=|\\vec{AC}| |\\vec{AB}| = D$ -\u0026gt; $D/|\\vec{AB}| = |\\vec{AC}|$ 求比率：$k = |\\vec{AC}|/|\\vec{AB}| = D/{|\\vec{AB}|^2}$ 最终坐标根据$A$和$k$可以求得：$C = A + k\\vec{AB}$ 如何验证C是投影点：\n验证其共线性：$\\vec{AC} = k \\vec{AB}$或$\\vec{AC} \\times \\vec{AB}=\\vec{0}$ 验证垂直：$\\vec{PC}\\cdot \\vec{AB} = 0$ 如何计算点P到线AB的距离d\n叉积的范数是由两个向量张成的平行四边形的面积$（\\vec{AB} \\times \\vec{AP}）$ 基于几何原理，这个面积也等于距离（高）乘以边长 $d * |\\vec{AB}|$ 所以$d = |\\vec{AB} \\times \\vec{AP}|/|\\vec{AB}|$ 3 代码实现 第一个版本代码，不用额外的库，手搓一些Utility函数，透彻了解原理：\n1#include\u0026lt;iostream\u0026gt; 2#include\u0026lt;cmath\u0026gt; 3using namespace std; 4 5struct Point 6{ 7 double x, y, z; 8 // Overloading the multiplication operator 9 Point operator*(double k) const 10 { 11 return {k*x, k*y, k*z}; 12 } 13 Point operator+(Point A) const 14 { 15 return {A.x + x, A.y + y, A.z + z}; 16 } 17 bool operator==(Point A) const 18 { 19 if (A.x == x and A.y == y and A.z == z) 20 { 21 return true; 22 } 23 else 24 { 25 return false; 26 } 27 } 28}; 29 30double dotProduct(Point A, Point B) 31{ 32 return A.x * B.x + A.y * B.y + A.z * B.z; 33} 34 35Point crossProduct(Point A, Point B) 36{ 37 return {A.y * B.z - A.z * B.y, 38 A.x * B.z - A.z * B.x, 39 A.x * B.y - A.y * B.x}; 40} 41 42float calcNorm(Point A) 43{ 44 return sqrt(A.x * A.x + A.y * A.y + A.z * A.z); 45} 46 47Point calcProjection(Point A, Point B, Point P) 48{ 49 Point AB = {B.x - A.x, B.y - A.y, B.z - A.z}; 50 Point AP = {P.x - A.x, P.y - A.y, P.z - A.z}; 51 double dot_product = dotProduct(AB, AP); 52 double k = dot_product / dotProduct(AB, AB); 53 Point C = A + (AB * k); 54 return C; 55} 56 57bool verifyProjection(Point A, Point B, Point P, Point C) 58{ 59 Point AC = {C.x - A.x, C.y - A.y, C.z - A.z}; 60 Point AB = {B.x - A.x, B.y - A.y, B.z - A.z}; 61 Point PC = {C.x - P.x, C.y - P.y, C.z - P.z}; 62 double dot_product = dotProduct(PC, AB); 63 Point cross_product = crossProduct(AC, AB); 64 Point zero_vec = {0, 0, 0}; 65 if (dot_product == 0 and cross_product == zero_vec) 66 { 67 return true; 68 } 69 else 70 { 71 return false; 72 } 73} 74 75float calcDistance(Point A, Point B, Point P) 76{ 77 Point AB = {B.x - A.x, B.y - A.y, B.z - A.z}; 78 Point AP = {P.x - A.x, P.y - A.y, P.z - A.z}; 79 Point cross_product = crossProduct(AB, AP); 80 float area_parallelogram = calcNorm(cross_product); 81 return (area_parallelogram / calcNorm(AB)); 82} 83 84int main() 85{ 86 // Line segment AB 87 Point A = {0, 0, 0}; 88 Point B = {4, 0, 0}; 89 // Point P 90 Point P = {5, 8, 0}; 91 // Project P to AB and get point C 92 Point C = calcProjection(A, B, P); 93 cout \u0026lt;\u0026lt; \u0026#34;Projection Point C: (\u0026#34; \u0026lt;\u0026lt; C.x \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; C.y \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; C.z \u0026lt;\u0026lt; \u0026#34;)\u0026#34; \u0026lt;\u0026lt; endl; 94 if (verifyProjection(A, B, P, C)) 95 { 96 cout \u0026lt;\u0026lt; \u0026#34;Correct!\u0026#34; \u0026lt;\u0026lt; endl; 97 } 98 else 99 { 100 cout \u0026lt;\u0026lt; \u0026#34;Incorrect.\u0026#34; \u0026lt;\u0026lt; endl; 101 } 102 cout \u0026lt;\u0026lt; \u0026#34;Distance from P to AB is: \u0026#34; \u0026lt;\u0026lt; calcDistance(A, B, P) \u0026lt;\u0026lt; endl; 103 return 0; 104} 另外一个版本，通过使用Eigen库来避免自己写Utility函数，行数大大减少（君子生非异也，善假于物也。）：\n1#include \u0026lt;iostream\u0026gt; 2#include \u0026lt;Eigen/Dense\u0026gt; 3using namespace std; 4using namespace Eigen; 5 6Vector3d calcProjection(Vector3d A, Vector3d B, Vector3d P) 7{ 8 Vector3d AB = B - A; 9 Vector3d AP = P - A; 10 float AC_norm = AB.dot(AP) / AB.norm(); 11 Vector3d C = A + AC_norm / AB.norm() * AB; 12 return C; 13} 14 15bool verifyProjection(Vector3d A, Vector3d B, Vector3d P, Vector3d C) 16{ 17 Vector3d AB = B - A; 18 Vector3d PC = P - C; 19 Vector3d AC = C - A; 20 Vector3d zero_vec = {0, 0, 0}; 21 Vector3d cross_product = AB.cross(AC); 22 float dot_product = PC.dot(AB); 23 if (dot_product == 0 and cross_product == zero_vec) 24 { 25 return true; 26 } 27 else 28 { 29 return false; 30 } 31} 32 33float calcDistance(const Vector3d A, const Vector3d B, const Vector3d P) 34{ 35 Vector3d AB = B - A; 36 Vector3d AP = P - A; 37 Vector3d cross_product = AB.cross(AP); 38 float area_parallelogram = cross_product.norm(); 39 return area_parallelogram / AB.norm(); 40} 41 42int main() 43{ 44 Eigen::Vector3d A = {0, 0, 0}; 45 Eigen::Vector3d B = {2, 0, 0}; 46 Eigen::Vector3d P = {1, 3, 0}; 47 48 Vector3d C = calcProjection(A, B, P); 49 cout \u0026lt;\u0026lt; \u0026#34;Projection point is: \u0026#34; \u0026lt;\u0026lt; C.x() \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; C.y() \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; C.z() \u0026lt;\u0026lt; endl; 50 if (verifyProjection(A, B, P, C)) 51 { 52 cout \u0026lt;\u0026lt; \u0026#34;Verification passes!\u0026#34; \u0026lt;\u0026lt; endl; 53 } 54 else 55 { 56 cout \u0026lt;\u0026lt; \u0026#34;Verification failed.\u0026#34; \u0026lt;\u0026lt; endl; 57 } 58 cout \u0026lt;\u0026lt; \u0026#34;Distance from P to AB is: \u0026#34; \u0026lt;\u0026lt; calcDistance(A, B, P) \u0026lt;\u0026lt; endl; 59} 有问题欢迎一起评论交流，我会回复或更新文章，谢谢！\n上述步骤的代码源文件：\n原始实现 使用Eigen库的实现 ","link":"https://mengwoods.github.io/cn/post/math/001-vector-cross-product/","section":"post","tags":null,"title":"向量点乘与叉乘"},{"body":"本文收集了一些有用的Docker命令。\n0. 基本概念和命令 Docker容器和Docker镜像\nDocker container 是Docker镜像的运行实例，提供了一个可以启动、停止和删除的独立环境。 Docker image 是一个静态、只读文件，包含创建容器所需的一切内容。它是启动Docker容器的源文件。\n基本Docker命令\n1# 通过镜像名拉取Docker镜像 2docker pull \u0026lt;镜像名称\u0026gt; 3 4# 列出本地可用的Docker镜像（镜像ID） 5docker images 6# 列出当前正在运行的Docker容器（容器ID） 7docker ps 8# 列出当前正在运行的Docker容器，包括已停止的容器 9docker ps -a 10 11# 查看特定容器的日志 12docker logs \u0026lt;容器ID\u0026gt; 13# 停止运行中的容器 14docker stop \u0026lt;容器ID\u0026gt; 15 16# 通过镜像ID在本地删除Docker镜像 17docker rmi \u0026lt;镜像ID\u0026gt; 18 19# 在`docker-compose.yml`文件中启动容器（使用-d以进行分离模式） 20docker-compose up 21# 在`docker-compose.yml`文件中停止容器 22docker-compose down 查看基本Docker信息 查看用户名和Docker镜像库：\n1docker system info | grep -E \u0026#39;Username|Registry\u0026#39; 1. 其他实用的命令 将镜像保存到.tar文件。\n首先使用docker images命令来查看您想要保存的镜像ID，然后使用以下命令进行保存：\n1docker save -o /path/to/\u0026lt;filename\u0026gt;.tar image-id image-id可以替换为REPOSITORY:TAG格式。为了更清晰的文件名，建议使用REPOSITORY:TAG格式命名文件。\n从.tar文件加载Docker镜像\n1docker load -i /path/to/filename.tar 重新标记Docker镜像\n1docker tag \u0026lt;source-repository:tag\u0026gt; \u0026lt;new-repository:tag\u0026gt; 一次性停止所有正在运行的Docker容器。\n往往在后台运行多个容器。如果不检查，很难意识到它们正在运行。要一次性停止它们，使用以下命令。该命令较为常用，建议设置为alias。\n1docker stop $(docker ps -a -q) 删除所有Docker镜像。\nDocker镜像会占用存储空间。要从系统中删除所有镜像，使用以下命令：\n1docker rmi $(docker images -q) 删除具有多个标签的同名镜像。\n如果要删除特定仓库中具有不同标签的所有镜像，请使用以下脚本。\n首先使用docker images命令来查看要删除的镜像。输出如下所示：\n1REPOSITORY TAG IMAGE ID CREATED SIZE 2registry-A 0.0.1 xxxx 25 hours ago 7.5GB 3registry-A 0.0.2 xxxx 25 hours ago 7.5GB 4registry-A 0.1.0 xxxx 25 hours ago 7.5GB 5... ... ... ... ... 在这种情况下，需要删除来自仓库registry-A的所有镜像。将下面的脚本保存到一个文件中，例如remove_tags.sh。\n1 #!/bin/bash 2 3 read -p \u0026#34;Enter the name of the repository: \u0026#34; REPOSITORY 4 5 # Get all tags for the repository (exclude headers) 6 tags=$(docker images --format \u0026#34;{{.Tag}}\u0026#34; $REPOSITORY) 7 8 # Loop through each tag and remove the corresponding image 9 for tag in $tags; do 10 docker rmi $REPOSITORY:$tag 11 echo \u0026#34;$REPOSITORY:$tag removed\u0026#34; 12 done 要使用它，在终端中运行bash remove_tags.sh，然后在提示时提供仓库名称。在此示例中是registry-A。此脚本将逐个删除指定仓库的所有标签镜像。\n","link":"https://mengwoods.github.io/cn/post/tech/003-some-docker-commands/cn/","section":"post","tags":null,"title":"一些实用的Docker命令"},{"body":"Dockerfile, Conda\n使用常规方法创建Conda虚拟环境并激活的过程会遇到很多错误，经过查找资料，总结了可行的方法。本文方法的参考文章。\n1. 保存虚拟环境yml文件 在本地激活你想放入Docker中的Conda虚拟环境：conda activate \u0026lt;myenv\u0026gt;。 输出Conda环境配置文件：conda env export \u0026gt; environment.yml。 删除配置文件中位于pip之后的内容。 在我的Docker生成过程中，环境配置文件中若包含pip安装包，则会报错。所以需要删除相关的包，在Dockerfile中，可以在conda环境激活后重新使用pip手动安装所需的。 下面是一个典型的environment.yml文件： 1name: myenv 2channels: 3 - pytorch 4 - nvidia 5 - ... 6dependencies: 7 - _libgcc_mutex=0.1=main 8 - blas=1.0=mkl 9 - ... 10- pip: # 删除此行及以下的内容并保存文件 11 - addict==2.4.0 12 - aliyun-python-sdk-core==2.14.0 13 - ... 14prefix: /home/*/miniconda3/envs/myenv 2. 编写Dockerfile 在你的Dockerfile中添加如下内容：\n1# 使用带有conda的镜像 2FROM continuumio/miniconda3 3 4WORKDIR /app 5 6# 使用上一节的环境文件创建虚拟环境: 7COPY environment.yml . 8RUN conda env create -f environment.yml 9 10# 激活该虚拟环境: 11SHELL [\u0026#34;conda\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;\u0026lt;myenv\u0026gt;\u0026#34;, \u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] 12 13# 若需要安装pip包，如下： 14RUN pip install A B C 通过该Dockerfile则可以构建使用Conda虚拟环境的镜像。\n","link":"https://mengwoods.github.io/cn/post/tech/002-run-conda-in-dockerfile/","section":"post","tags":null,"title":"在Docker中使用conda虚拟环境"},{"body":"准备 本安装在以下环境中进行了测试：Ubuntu 20.04、CUDA-11.6、PyTorch v1.13.1。\nMiniconda 此安装在conda虚拟环境中进行，因此请确保已安装Miniconda。\nLinux下安装：\n1mkdir -p ~/miniconda3 2wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh 3bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 4rm -rf ~/miniconda3/miniconda.sh 初始化：\n1~/miniconda3/bin/conda init bash 2~/miniconda3/bin/conda init zsh CUDA Toolkit 本问建议使用CUDA 11.6和Nvidia驱动程序510.39.01。Nvidia驱动程序可以与CUDA一起安装。\n检查CUDA版本：nvcc --version 从系统中删除CUDA：sudo /usr/local/cuda-11.x/bin/cuda-uninstaller（将x替换为CUDA的版本） 检查Nvidia驱动程序版本：nvidia-smi 删除Nvidia驱动程序：sudo /usr/bin/nvidia-uninstall 安装CUDA11.6和驱动程序510.39.01：\n1wget https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda_11.6.0_510.39.01_linux.run 2sudo sh cuda_11.6.0_510.39.01_linux.run 在终端中勾选Nvidia驱动程序。\n若需要安装其他版本，请查看官方网站。\n安装依赖 创建一个虚拟环境：\n1conda create --name openmmlab python=3.8 -y 2conda activate openmmlab 安装PyTorch v1.13.1：\n1conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia 安装SpConv：\n1pip install spconv-cu116 安装MMDetection3D 了解更多信息：https://mmdetection3d.readthedocs.io/en/latest/get_started.html\n安装MM相关依赖 1# 安装mim 2pip install -U openmim 3mim install mmengine \u0026#39;mmcv\u0026gt;=2.0.0rc4\u0026#39; \u0026#39;mmdet\u0026gt;=3.0.0\u0026#39; 安装MMDetection3D库 情况一: 使用该库作为一个第三方包：\n1# 安装mmdet3d 2mim install \u0026#34;mmdet3d\u0026gt;=1.1.0\u0026#34; 情况二：以源码方式安装：\n1git clone https://github.com/open-mmlab/mmdetection3d.git -b dev-1.x 2cd mmdetection3d 3pip install -v -e . 测试 要测试结果，请在Python环境中尝试从中导入库，例如：\n1from mmdet3d.apis import init_model, inference_detector 或者\n1import mmdet3d 2print(mmdet3d.__version__) ","link":"https://mengwoods.github.io/cn/post/tech/001-install-openmm3d-lib/cn/","section":"post","tags":null,"title":"MMDetection3D库的安装指导"},{"body":"安装Hugo 在Github releases页面下载安装最新版本。 例如下载*.deb文件然后使用dpkg -i *.deb安装。\n或者使用官方文档安装：【安装方法】\n第一步：创建所需Repo 在Github创建两个Repo，创建时皆勾选Add a README file选项：\nBlog源文件仓库：存储网页源文件，用于生成博客网站。（参考命名：blog-resources） Pages仓库：用于保存所生成的网页文件，并可通过网址进入博客。（命名需按照：\u0026lt;username\u0026gt;.github.io，其中\u0026lt;username\u0026gt;为Github账户名。【官方说明】） 第二步：使用Hugo命令创建博客文件 源文件仓库名以blog-resources为例。\n克隆blog-resources到本地。 进入blog-resources路径中，使用Hugo命令创建网站整体文件夹结构hugo new site \u0026lt;blog-name\u0026gt;，替换\u0026lt;blog-name\u0026gt;为你需要的，例如woods-blog。 此时repo文件结构以及它们的主要用途如下： 1├── README.md # 创建Repo时自动天剑的README 2└── woods-blog # 博客文件夹名，与上一步使用hugo创建的一致 3 ├── archetypes 4 ├── config.toml # 博客网站自定义配置文件，需要进一步编辑 5 ├── content # 博客内容文件夹，存放博客文档，图片等 6 ├── data 7 ├── layouts 8 ├── resources 9 ├── static 10 └── themes # 主题文件夹，需要进一步安装Hugo主题 第三步：添加主题文件并配置 浏览Hugo主题并选择。 主题介绍部分一般带有安装方法，可以按照该方法进行安装。但我在安装一些主题过程中总遇到错误。此处以我选择的主题Mainroad进行介绍。 进入博客文件路径blog-resources/woods-blog，以git submodule方式添加主题文件。 1git submodule add https://github.com/vimux/mainroad.git themes/mainroad 若添加成功，此时在blog-resources/woods-blog/themes下会出现该主题文件名为Mainroad。 1└── themes 2 └── Mainroad # 此处为所安装的主题名 在blog-resources/woods-blog/themes/Mainroad/exampleSite中，复制文件夹content，static，和文件config.toml到blog-resources/woods-blog下，并覆盖原有文件。 配置blog-resources/woods-blog/config.toml文件，主要需要查看的关键词如下： 1# 确认url为所创建Pages Repo网址，其形式如下，结尾带`/` 2baseurl = \u0026#34;https://\u0026lt;username\u0026gt;.github.io/\u0026#34; 3# Theme 为所安装的主题名字 4theme = \u0026#34;mainroad\u0026#34; 第四步：测试博客并发布为HTML 启动Hugo server：hugo server，登录网址http://localhost:1313/预览博客主题。 若需要添加博客文档，可在blog-resources/woods-blog/content下创建新的.md文档进行测试，Hugo server会实时更新以预览。 发布博客为HTML网页，在blog-resources/woods-blog/路径下使用命令hugo。 若发布成功，此时在路径blog-resources/woods-blog/下会多一个文件夹public用于保存HTML网页文件。 第五步：将HTML文件推送到Pages仓库 进入blog-resources/woods-blog/public路径，将该文件夹设置为Pages仓库：\n1# 初始化为新的Git仓库 2git init 3# 创建main分支 4git checkout -b \u0026#39;main\u0026#39; 5# 将Pages仓库的SSH添加为到本地 6git remote add origin git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git 推送本地文件到仓库：\n1# 同步本地与远端的commits 2git pull --rebase origin main 3# 添加本地所有的改变 4git add . 5# 添加commit以及相关描述信息 6git commit -m \u0026#34;commit information, e.g. add blog template\u0026#34; 7# 推送到远端仓库 8git push origin main 浏览器键入网址以浏览博客https://\u0026lt;username\u0026gt;.github.io/，可能需要等待几分钟。\n后续 推送blog-resources仓库至Github。\n参考 如何用 GitHub Pages + Hugo 搭建个人博客。 ","link":"https://mengwoods.github.io/cn/post/tech/000-build-hugo-site/en/","section":"post","tags":["Hugo"],"title":"使用Hugo搭建个人博客"},{"body":"","link":"https://mengwoods.github.io/cn/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://mengwoods.github.io/cn/categories/dl/","section":"categories","tags":null,"title":"DL"},{"body":"","link":"https://mengwoods.github.io/cn/","section":"","tags":null,"title":"Menghao blog"},{"body":"","link":"https://mengwoods.github.io/cn/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://mengwoods.github.io/cn/categories/math/","section":"categories","tags":null,"title":"Math"},{"body":"","link":"https://mengwoods.github.io/cn/categories/code/","section":"categories","tags":null,"title":"Code"},{"body":"简介 《谷物大脑》（Grain Brain）【豆瓣】一书由美国神经科医生、营养学家 David Perlmutter 撰写。该书主要观点为食物中的麸质（Gluten）以及过量的碳水化合物会对身体造成持久的损害，二者也是间接导致糖尿病以及阿尔兹海默症的诱因；而食用健康的脂肪、蛋白质、维生素等则对身体及大脑健康有益。作者在饮食方面的建议是减少对谷物的摄入，食用无麸质食物，降低每日碳水化合物食品摄入，提高肉、蛋、蔬菜等摄入。\n何为麸质？ 麸质是一类蛋白质，存在与许多谷物中，如大麦、小麦、燕麦等。其负责提供面类食物的韧性和弹性，我们所熟知的面筋就是麸质，完全由麸质构成的食品主要有辣条、牛筋面等。麸质蛋白与其他蛋白质不同，人类肠道无法正常消化它，而且其容易粘到小肠壁上造成一些健康问题，部分人对麸质会产生直接过敏反应。面类食品中广泛含有麸质，如面包、披萨、意大利面、面条、包子饺子等。即有面的地方就有麸质，除非原材料是去麸质处理过的。\n有关碳水化合物 碳水化合物（carbohydrate）主要指糖类食物，可以为身体提供能量。淀粉是食物中常见的糖，谷物、根茎类食物含有大量淀粉，部分蔬菜中也含有少量淀粉。水果中含有果糖葡萄糖等，这些都会提高血糖从而为身体提供能量。\n在看食物碳水化合物比例时，还要同时注意其升糖指数（glycemic index， GI），其度量食物在体内的升糖能力。尽管一些蔬菜中含有淀粉，但由于其被纤维所包围，在体内会被缓慢地消化，所以其GI相比较米饭而言是较低的。米饭面包等食物中的淀粉则会被迅速地消化，具有较高的GI。低GI食物对身体有益，而长期食用高GI食物则会对身体造成一些问题。一个有趣的点是即使是同样的食物，不同的烹饪形式也会影响其GI，相比热腾腾的米饭而言，冷的寿司就具有较低的GI，因为冷的寿司在胃里会消化更久。\n摒弃的食物 含有麸质食物：面类食品（面包、面条、意大利面、糕点、麦片、披萨、包子、饺子、油条等），含有麸质的调料：大豆酱油等。 不健康的油类：人造黄油、植物起酥油以及任何商业品牌的食用油（大豆油、玉米油、棉籽油、油菜籽油、花生油、红花籽油、葡萄籽油、葵花籽油、稻糠油还有小麦胚芽油） 加工过的含糖食物：薯片、饼干、曲奇饼、糕点、松饼、比萨面团、蛋糕、甜甜圈、含糖的零食、糖果、能量棒、冰激凌/冷冻酸奶冰激凌/果汁牛奶冻、果酱/果冻/蜜饯、西红柿酱、涂抹型再制干酪、果汁、果干、运动饮料、软饮料/汽水、油炸食品、蜂蜜、龙舌兰、糖（白糖和红糖）、玉米糖浆以及枫树糖浆。 淀粉食物：玉米、山药、土豆、红薯（我感觉这些应该属于少量吃的一组） 非发酵的大豆（如豆腐和豆浆）和加工过的大豆制品（请查找成分列表中有“大豆分离蛋白”的食品；避免大豆奶酪、大豆汉堡包、大豆热狗、大豆鸡块、大豆冰激凌、大豆酸奶）。请注意：虽然一些自然酿造酱油中理应不含麸质，但是许多商业品牌的酱油中有微量麸质。如果你在烹饪中需要使用酱油，那么请选择用100%大豆酿造不含小麦成分的酱油。 个人感觉淀粉类食物应该输入少量食用的一类。而关于非发酵的大豆类，我并不懂其原因。\n需要的食物 健康的油脂：特级初榨橄榄油、芝麻油、椰子油、草饲牛油和有机或者牧场黄油、印度酥油、杏仁乳、鳄梨、椰子、橄榄、坚果和坚果酱、奶酪（除了蓝纹奶酪之外）以及部分种子〔亚麻籽、葵花籽、南瓜籽、芝麻、奇异籽(Chia Seeds)〕。（注：花生不属于坚果） 蛋白质：全蛋，野生鱼类〔三文鱼、裸盖鱼(Black Cod)、鲯鳅鱼(Mahi Mahi)、石斑鱼(Grouper)、鲱鱼(Herring)、鳟鱼(Trout)、沙丁鱼〕贝类以及软体动物（虾、蟹、龙虾、贻贝、蛤、牡蛎），草饲肉、禽类以及猪肉（牛肉、羊肉、肝脏、野牛肉、鸡肉、火鸡肉、鸭肉、鸵鸟肉、小牛肉），野味。 蔬菜：绿叶蔬菜和莴苣、羽衣甘蓝、菠菜、西兰花、甜菜、卷心菜、洋葱、蘑菇、花椰菜、抱子甘蓝、（酸）泡菜、朝鲜蓟、苜蓿芽(Alfalfa Sprouts)、青刀豆、芹菜、小白菜(Bok Choy)、小红萝卜、豆瓣菜(Watercress)、萝卜(Turnip)、芦笋、大蒜、韭菜、茴香、青葱、葱、姜、豆薯(Jicama)、欧芹、荸荠。 低糖水果：鳄梨、灯笼椒、黄瓜、西红柿、西葫芦、笋瓜(Squash)、南瓜、茄子、柠檬、酸橙。 药草、调料和佐料：只要你仔细查看了标签，那么你可以随意使用调料和佐料。吻别西红柿酱和酸辣酱吧，开始享用芥末酱、辣根酱(Horseradish)、橄榄酱(Tapenade)、墨西哥辣椒酱(Salsa)，不过必须是其中不含麸质、小麦、大豆和糖的酱料。对药草和调料基本没有限制，但是请留意包装的产品，因为出产该产品的工厂可能也加工小麦和大豆。 鸡蛋：鸡蛋非常健康，可以作为早餐或者零食，可以多吃。 适当食用的食物 一天一次使用少量，或者理想情况下每星期使用几次：\n不含麸质的谷物：苋菜、荞麦、大米（糙米、白米、野生米）、小米、藜麦(Quinoa)、高粱、画眉草。（关于燕麦的注解：虽然燕麦本身不含麸质，但是因为加工燕麦的磨坊也加工小麦而经常会使燕麦中混有麸质，除非是保证不含麸质的燕麦，否则请避免使用燕麦。）不含麸质的谷物在为了人类食用而进行加工的过程中（例如，整粒燕麦的研磨还有大米包装前的加工），谷物的物理结构方式改变，这会增加炎症反应的风险。出于这一原因，我们要限制这些食品。 豆类：（豆子、小扁豆和豌豆）。例外：你可以吃鹰嘴豆泥（由鹰嘴豆制成）。 蔬菜：胡萝卜和欧防风(Parsnips)。 甜的水果（整果）：浆果最佳，要格外当心含糖的水果，比如杏、芒果、甜瓜、木瓜、李子和菠萝。 牛奶和牛奶制作的奶油：在烹调、咖啡和茶中少量使用。 白软干酪(Cottage Cheese)、酸奶和酸牛乳酒(Kefir)：在烹调或者浇汁中少量使用。 甜味剂：天然甜菊(Stevia)和巧克力（请选择可可含量在70%或者以上的黑巧克力）。 酒：如果你想喝，那么一天一杯，首选红酒。 补剂以及其他建议 补剂：此外需要人们需要从额外的补剂中获取营养，这包含：DHA（一种欧米伽3脂肪酸，存在于鱼油中），白藜芦醇（resveratrol），姜黄（Trumeric or Curcuma longa），椰子油，a-硫辛酸（lipoic acid），维生素D\n禁食：有计划的禁食可以帮助身体消耗储存在内脏中的糖和脂肪，帮助身体更加健康。书中建议一年至少禁食4次，在换季的时候进行（例如，9月、12月、3月以及6月的最后一个星期）。一次24-72小时。禁食期间可以喝大量的水。\n运动：每天要保持至少20分钟的有氧运动。\n其他 该书提出的理念可以作为一个饮食参考，其建议与生酮饮食、地中海饮食有相似之处，低碳水的饮食模式符合现代人的生活方式。然而我认为也不必过于极端地按照上述食谱进食，因为人体所需的部分营养也依赖于部分谷物，找到适合自己的方式最重要。\n","link":"https://mengwoods.github.io/cn/post/hobby/2023-12-08-grain-brain/","section":"post","tags":null,"title":"《谷物大脑》总结"},{"body":"","link":"https://mengwoods.github.io/cn/categories/hobby/","section":"categories","tags":null,"title":"Hobby"},{"body":"","link":"https://mengwoods.github.io/cn/categories/reading/","section":"categories","tags":null,"title":"Reading"},{"body":"","link":"https://mengwoods.github.io/cn/tags/andriod/","section":"tags","tags":null,"title":"Andriod"},{"body":"","link":"https://mengwoods.github.io/cn/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://mengwoods.github.io/cn/categories/tech/","section":"categories","tags":null,"title":"Tech"},{"body":"Andriod 项目目录各个文件夹功能详细记录\n当使用 Android Studio 新建一个工程时，会出现如下图所示的目录结构：\n自上而下各个文件夹功能的介绍如下：\n.gradle 文件夹是 Gradle 构建系统生成的目录，用于存储构建过程中的各种临时文件和缓存文件。这个目录通常不需要手动干预，Gradle 会自动管理其中的文件。\nGradle 是一种易于使用的构建工具，用于自动化编译、打包、测试和部署软件项目。其支持多种编程语言，包括 Java、Kotlin、Groovy 等。 在安卓开发过程中其可通过一系列构建规则帮助开发 app 的过程。在创建工程时，Android Studio 会根据默认的通用构建规则来完成 app 的构建。 .idea 包含 Android Studio 项目的配置文件，包括项目的工作区设置和其他配置信息，例如工作区布局、运行配置、代码风格等信息。由于这些信息是 IDE 特定的，与项目源代码和构建输出等内容分开存储，因此使用了一个隐藏的目录。文件夹命名源于开发环境 IntelliJ IDEA 的名字。\napp 为项目的主要工作目录，包含项目的源代码以及资源文件，如 app 布局、图像等，以及程序的元数据。\nlibs 用于存放应用程序所使用的第三方库（JAR 文件）。 Java Archive（JAR）是一种压缩文件格式，通常用于将一组相关的类文件、资源文件和元数据打包到一个单独的文件中。 src/androidTest 包含用于执行 Android 测试的代码。这里的测试主要是针对应用程序的功能和用户界面的集成测试。 src/main/java 中包含项目的源代码，通常以包的形式组织在这个目录下。Android 应用的主要逻辑和功能实现都在这里。 src/main/res 中包含资源文件，如 app 布局、字符串、图像等。定义了应用程序的用户界面和其他非代码相关的方面。 src/main/AndroidManifest.xml 应用程序的清单文件，其中包含了有关应用程序的元数据，如应用程序的包名、版本号、权限声明、组件声明（活动、服务、接收器等）等。这个文件是 Android 系统了解应用程序结构和要求的重要文件。 src/test 应用程序的单元测试代码。这里的测试主要是针对应用程序中各个单元（类、方法等）的功能进行测试。 build.gradle 该文件是 Gradle 构建文件，用于配置 app 模块的构建过程。其中包含了应用程序的依赖项、编译选项、签名配置等。 proguard-rules.pro ProGuard 是一个代码缩小（minification）和混淆工具，用于减小 APK 文件的大小并增加安全性。该文件用于配置 ProGuard 规则，指定哪些代码应该被缩小或混淆。 gradle 包含 Gradle 构建系统的配置文件\n其他文件：\n.gitignore Git 版本控制系统的文件，用于排除某个路径，不被 Git 所记录。 .build.gradle 根级别的 Gradle 构建文件，用于配置整个项目的构建过程。这个文件通常包含项目的全局配置，例如项目的名称、版本、仓库地址等。 .gradle.properties 包含一些项目级别的 Gradle 属性配置。可以在这里设置一些全局的 Gradle 配置属性，例如 Gradle 版本、Java 版本等。 .gradlew 和 .gradlew.bat 这是 Gradle Wrapper 的脚本文件，用于在项目中运行 Gradle 而不需要事先安装 Gradle。.gradlew 用于 Unix/Linux 系统，而 .gradlew.bat 用于 Windows 系统。通过运行这些脚本，Gradle 会自动下载和使用指定版本的 Gradle。 local.properties 包含本地配置信息，例如 Android SDK 的路径。这个文件通常是本地特定的。 settings.gradle 包含有关项目模块的配置信息，例如模块的名称。这个文件允许你指定项目中包含哪些模块，以及这些模块的名称。 其他文件夹：\nExternal Libraries 主要包含了项目依赖的外部库和框架的信息。当使用 Gradle 或其他构建工具导入依赖项时，这些依赖项将被列在这里，以便查看、浏览项目所使用的所有外部库的版本和结构。这个视图是一个便捷的方式了解所有第三方库。 Scratches and Consoles 包含了一些临时性的操作的结果，以及控制台（Console）的输出。Scratches 用于创建和保存临时性的代码片段，类似于草稿纸，可以方便地测试一些代码片段或进行快速的实验。Console 包含了各种控制台输出，例如 Gradle 构建过程的输出、运行应用程序时的日志信息等。不同类型的控制台（如 Gradle、Logcat）都会有相应的选项卡，以便查看不同方面的输出 ","link":"https://mengwoods.github.io/cn/post/tech/004-as-folder-functios/cn/","section":"post","tags":["Andriod"],"title":"了解 Andriod Studio 项目目录各文件功能"},{"body":"Google公司的AK-47 在吴军博士《数学之美》一书中的第13章谈到了Google公司的阿米特辛格（Amit Singhal）博士。Singhal博士被誉为是Google的AK-47，因为其经常能够准确地采用简洁的方法并以超高的效率来解决工程中面临的难题。AK-47突击步枪被誉为枪王之王，该枪的显著优点有：结构简单，制造成本低，易上手操作，故障率低，不易损坏，杀伤力大，且适用于多种苛刻的环境。对于战场环境而言，AK-47的优点让它光芒万丈。作者认为在计算机科学行业，一个优秀的算法也应该具备类似AK-47的特点：简洁，有效，可靠性高而且容易被人读懂。\n吴军博士在与Singhal博士在Google共事时期，有一次面对如何解决网络搜索中的作弊问题，这需要组建一个精确的分类器，来准确区分一个搜索是否具有商业意图，从而推测该搜索有多大的可能性包含作弊手段。由于作者深厚的学术经历，面临问题时习惯于寻找一个完美的解决方案，当然这需要耗费大量时间来完成。然而即使设计一个可用的且功能较为完善的分类器，大约也需要三、四个月的时间。此时Singhal博士却提出要使用最简单的可行的方案，这样就能够把工作量急剧减少到一个周末。Singhal博士的工作理念是对于工程问题，最简单实用的方法是最好的方法。在二人采用这个只用了一个周完成的反作弊方法后，一两个月后搜索作弊的数量就减少了一半，并且该分类器非常小巧且运行快速，由于其结构足够的简单，使得后期维护也较为容易。Google公司采用了很久这项技术并最终申请了专利。很明显该方法经得起了时间的考验，并取得了不错的反作弊效果。\n那么为什么在工程中最简单可行的办法会具有不错的实际效果？这个话题是值得探讨的。\n“简单与可行” 在工业领内，最小可行产品（MVP）是一个经常出现的名词。人们在设计产品雏形时，往往按照合同最低要求设计出MVP方案并得以执行，这可以让开发难度和速度得到控制。在MVP实现后再后续进行调优与功能丰富。这种理念可以尽可能地缩短工作进度，让产品迭代速度更快，对产品本身而言，其质量也会随着迭代次数的增加而迅速升高。这应该便是简单可行理念下的产物。\n举一个生活中的“简单可行”例子，想在工作或学习时不被手机吸引从而影响效率，一个简单可行的方法是每次开始工作、学习之前，把手机放到一个你看不到的地方比如衣柜里。这个看似简单的办法往往有出奇的效果，有时工作了一两个小时都不曾有过看一眼手机的念头。这个方法首先足够的简单，简单到没试过的人们会怀疑这是否有真的有效果，其次它并没有从根本上解决人容易被手机吸引的本质，不算完美的解决方案，只能说这个方法解决了大部分问题。然而就是这个简单的方法，在实际生活中有着令人惊讶的效果，若能形成习惯，那么工作、学习的效率会大大提升。我认为这便是一个在生活中符合“简单“与“可行“理念的方法。\n接下来在以下几个方面探讨这种工作中的“简单与可行”理念。\n时间成本 与学术科研活动不同，工程中遇到的问题一般具有时间的紧迫性，这种情况下寻找一个问题的完美解决方案无疑是奢侈的。在如下的不可能三角中，时间少，成本低以及质量高是无法同时实现的。工程问题的解决方案追求的是时间少与成本低，那么就必须放弃对质量的高要求。在这种情况下必须在简单的一系列方法中寻找可行的，即使没有那么完美。这对工作者的要求必须是具备足够的工作经验，才能够在短时间内计划出一个足够简单的方法来尽其可能地解决面临的问题。而对于学术科研来说，其时间限制较少，在漫长的科研过程中，寻求完美的结果便是科研工作者的目标。\n解决效果 那么如果用时较少只使用简单的方案，能否解决实际问题呢？或是能多大程度地解决问题呢？\n当我们去解决一个困难时，所付出的时间和问题解决的程度并不是简单线性相关的关系，而是一个类似Log曲线的模型，如下图，其表示所付出的时间与解决问题的程度的关系。生活中很多事情的发展是符合这个规律的，比如在健身过程中，初期的锻炼肌肉增长速度通常较快，而随着时间推移肌肉增长速度会越来越慢，到了一定阶段后健身只能维持身体的肌肉量。当我们面临一个工程问题时，表面的浅显的问题一般是容易发现与解决的，同时耗时也小，随着简单层级问题的逐步解决，对深层次问题的分析就会变得越来越耗时间。然而当把比较容易解决的问题都解决掉时，实际效果会好一大截。\n如图中A，B，C三点之间的间隔时间相同，而问题解决的程度与投入时间的比例会逐渐下降。若想达到完美结果不知道要付出多少时间，或许永远无法达到。研究学术问题的学者多数处于C之后的阶段，他们投入大量的时间寻找某个领域中接近完美的算法，这也是为什么学术是耗时间的工作。而对于实际工程问题，能够达到A，B附近区域的方案往往会得到青睐，因为其在耗时可接受的情况下效果也不错。\n应用场景 除开众多优点外，AK-47的缺点也有很多：噪声大，后坐力强，外表粗糙不精致，瞄准精度低等，然而一把冲锋枪在战场上首先要保证的是不容易坏，且杀伤力强。满足了这两点后，其他方面甚至是可以不需过多考虑的。所以在解决实际问题时，应用场景也是需要考虑的。如果提出一个简洁的方法，但其可以最需要关注的方面进行一些改善调优，那么它便是在该应用场景下行之有效的方法。完美的方案除开时间成本之外，其可能在众多方面对系统进行全棉的改善，但在最需要关注的方面上，其效果可能并不比简单方法优秀多少。所以在针对应用场景上，使用简单的方法是高效的。\n可靠性 简单的方法之所以简单，是因为其逻辑步骤清晰，多数人容易理解。这带来的是维护便捷，维护人员不需要太高的知识储备。任何软件都需要随时间进行迭代，以修复Bug和提高软件的可靠性。而完美的深入的方法需要更多的知识储备，容易造成难以理解，这对工程项目来说，不利于日后的维护，维护成本也比较高。容易维护的方法在可靠性上是更高的。\n深奥的简洁 我们所学习过的物理公式如牛顿三大定律、爱因斯坦质能方程等，简洁地描述了这个世界的运行规律。这种简洁表达的背后是作者深厚的功底、无数次实验与思考的结果。对于本文例子中的“简单方法“，是Google公司知名博士口中的“简单”，其含义可能与普通人理解的“简单”并不相同。\n简单方法若想得到最佳效果，要保证其在某一方面带来效果的同时，不会在其它方面影响系统的正常运行。顾此失彼的简单方法不会是好方法。这意味着无论如何，都需要具有足够的知识储备与全局的把控能力才能提出工程中切实可行的简单方法。似乎使用“合适的方法”更为精准，简单只是它所表现出来的一个特征。让追求完美、持续学习与思考成为一种习惯，这样在面对实际工作问题时，才能够举重若轻、拨冗求简，寻求所谓的“简单”。\n","link":"https://mengwoods.github.io/cn/post/hobby/2023-11-15-beauty-of-math/cn/","section":"post","tags":null,"title":"《数学之美》中的Amit Singhal博士的工作理念"},{"body":"","link":"https://mengwoods.github.io/cn/categories/docker/","section":"categories","tags":null,"title":"Docker"},{"body":"","link":"https://mengwoods.github.io/cn/categories/ai/","section":"categories","tags":null,"title":"AI"},{"body":"","link":"https://mengwoods.github.io/cn/tags/hugo/","section":"tags","tags":null,"title":"Hugo"},{"body":"","link":"https://mengwoods.github.io/cn/categories/web/","section":"categories","tags":null,"title":"Web"},{"body":"","link":"https://mengwoods.github.io/cn/series/","section":"series","tags":null,"title":"Series"}]